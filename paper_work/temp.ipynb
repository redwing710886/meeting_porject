{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\name_list.txt\",'rb','utf8') as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "    with codecs.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\name_list.csv\",'wb','utf8') as g:\n",
    "        \n",
    "        g.write('名字,學號,信箱\\r\\n')\n",
    "    \n",
    "        for line in content:\n",
    "            line = line.split()\n",
    "            \n",
    "            g.write(line[0]+line[1]+','+str(line[2].split('@')[0])+','+line[2]+'\\r\\n')\n",
    "            \n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "connection = pymysql.connect(host=\"140.119.80.113\", user=\"user06\", passwd=\"yueboss\", db=\"dbo.CD2010\",charset='utf8')\n",
    "\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        sql = \"SELECT * FROM `word` WHERE `class`='認知-猜測 epistemic-conjecture'\" #基本sql指令\n",
    "        cursor.execute(sql)\n",
    "        \n",
    "        #提取出的資料必須以遞迴形式拿出\n",
    "        for i in cursor:\n",
    "            print (i)    \n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#取出SCS各文本\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\碩士班\\\\SNA\\\\meeting\\\\source\\\\SCS_4.0\\\\CKIP\\\\\"\n",
    "out_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\article\\\\'\n",
    "\n",
    "file_list = []\n",
    "count = 1\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    file_list.append(file)\n",
    "\n",
    "for file in file_list:\n",
    "    \n",
    "    tree = ET.parse(path+file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for article in root.iter('article'):\n",
    "        with codecs.open(out_path+str(count).zfill(5)+'.txt','wb','utf8') as g:\n",
    "            if article.find('title').text != None:\n",
    "                g.write('#'+article.find('title').text+'\\r\\n')\n",
    "            #print (article.find('title').text)\n",
    "            for i in article.find('text').findall('sentence'):\n",
    "                #print (i.text)\n",
    "                if i.text != None:\n",
    "                    g.write(i.text+'\\r\\n')\n",
    "            #time.sleep(1)\n",
    "        count += 1\n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "myfont = matplotlib.font_manager.FontProperties(fname='c:\\\\windows\\\\fonts\\\\msyh.ttc', size=14)\n",
    "\n",
    "#调节图形大小，宽，高\n",
    "plt.figure(figsize=(6,9))\n",
    "#定义饼状图的标签，标签是列表\n",
    "labels = ['第一部分','第二部分','第三部分']\n",
    "#每个标签占多大，会自动去算百分比\n",
    "sizes = [20.5,40.5,38.9]\n",
    "colors = ['red','yellowgreen','lightskyblue']\n",
    "#将某部分爆炸出来， 使用括号，将第一块分割出来，数值的大小是分割出来的与其他两块的间隙\n",
    "explode = (0,0.05,0)\n",
    "\n",
    "patches,l_text,p_text = plt.pie(sizes,explode=explode,labels=labels,colors=colors,\n",
    "                                labeldistance = 1.1,autopct = '%3.1f%%',shadow = False,\n",
    "                                startangle = 90,pctdistance = 0.6)\n",
    "\n",
    "#labeldistance，文本的位置离远点有多远，1.1指1.1倍半径的位置\n",
    "#autopct，圆里面的文本格式，%3.1f%%表示小数有三位，整数有一位的浮点数\n",
    "#shadow，饼是否有阴影\n",
    "#startangle，起始角度，0，表示从0开始逆时针转，为第一块。一般选择从90度开始比较好看\n",
    "#pctdistance，百分比的text离圆心的距离\n",
    "#patches, l_texts, p_texts，为了得到饼图的返回值，p_texts饼图内部文本的，l_texts饼图外label的文本\n",
    "\n",
    "#改变文本的大小\n",
    "#方法是把每一个text遍历。调用set_size方法设置它的属性\n",
    "for t in l_text:\n",
    "    t.set_size=(30)\n",
    "    t.set_fontproperties(myfont)\n",
    "for t in p_text:\n",
    "    t.set_size=(20)\n",
    "    #t.set_fontproperties(myfont)\n",
    "# 设置x，y轴刻度一致，这样饼图才能是圆的\n",
    "plt.axis('equal')\n",
    "plt.legend(prop=myfont)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql as db\n",
    "import operator\n",
    "import time\n",
    "\n",
    "PORT = 3306\n",
    "HOST = '140.119.164.170'\n",
    "USER = 'wcchen'\n",
    "PASSWORD ='62295'\n",
    "DB = input('DB = ')\n",
    "CHARSET = 'UTF8'\n",
    "USEUNICODE = True\n",
    "\n",
    "\n",
    "def m():\n",
    "    table = input('table = ')\n",
    "    data = database(table)\n",
    "    print(data)\n",
    "\n",
    "def database(table):#存取database\n",
    "    connection = db.Connection(host = HOST, port = PORT, user = USER, passwd = PASSWORD, db = DB, charset = CHARSET, use_unicode = USEUNICODE)\n",
    "    o = connection.cursor()\n",
    "    o.execute('SELECT * FROM `%s` WHERE 1' % table)\n",
    "    raw = o.fetchall()\n",
    "    return raw\n",
    "\n",
    "m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\\"\n",
    "\n",
    "count = 1\n",
    "error = []\n",
    "file = []\n",
    "with codecs.open(path+'文藝.csv','rb') as f:\n",
    "    head = f.readline()\n",
    "    content = f.readlines()\n",
    "    \n",
    "    for i in content:\n",
    "        try:\n",
    "            print (count,i.decode('big5').strip())\n",
    "            file.append(i.decode('big5').strip())\n",
    "        except:\n",
    "            print (count)\n",
    "            error.append(count)\n",
    "            file.append('')\n",
    "        count += 1\n",
    "\n",
    "with codecs.open(path+'temp.txt','wb','utf8') as g:\n",
    "    for i in file:\n",
    "        g.write(i+'\\r\\n')\n",
    "\n",
    "print (error)        \n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os \n",
    "\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\\"\n",
    "\n",
    "book = []\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for file in os.listdir(path+'雷震處理資料\\\\source\\\\雷震日記母體\\\\'):\n",
    "    file_list.append(file)\n",
    "    \n",
    "with codecs.open(path+'文藝.txt','rb','utf8') as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "        \n",
    "        book.append(line)\n",
    "\n",
    "size = len(book)\n",
    "\n",
    "out = []\n",
    "check = False\n",
    "        \n",
    "count = 0\n",
    "for file in file_list:\n",
    "    with codecs.open(path+'雷震處理資料\\\\source\\\\雷震日記母體\\\\'+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        for i in book:\n",
    "            if i.split(',')[0] in head and i.split(',')[1] in head:\n",
    "                if i in out:\n",
    "                    check = True\n",
    "                else:\n",
    "                    out.append(i)\n",
    "                print (i)\n",
    "                count += 1\n",
    "                break\n",
    "\n",
    "for i in book:\n",
    "    if i not in out:\n",
    "        print ('error',i)\n",
    "print (check,count,len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://www.jerrynest.com/python-get-ubike-opendata/\n",
    "#http://learn4rookie.blogspot.tw/2016/03/python-youbike.html\n",
    "import urllib.request\n",
    "import gzip\n",
    "import json\n",
    "#from pprint import pprint\n",
    "url = \"http://data.taipei/youbike\"\n",
    "urllib.request.urlretrieve(url, \"data.gz\")\n",
    "f = gzip.open('data.gz', 'r')\n",
    "jdata = f.read()\n",
    "f.close()\n",
    "data = json.loads(jdata.decode('utf8'))\n",
    "for key,value in data[\"retVal\"].items():\n",
    "    sno = value[\"sno\"]\n",
    "    sna = value[\"sna\"]\n",
    "    print (\"NO.\" + sno + \" \" + sna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\論文資料\\\\SCS\\\\\"\n",
    "out_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\SCS2\\\\\"\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    file_list.append(file)\n",
    "    \n",
    "\n",
    "p = IntProgress()\n",
    "p.max = len(file_list)\n",
    "p.description = 'start'\n",
    "display(p)\n",
    "count = 0\n",
    "\n",
    "for file in file_list:\n",
    "    with codecs.open(path+file,'rb','utf8') as f:\n",
    "        title = f.readline()\n",
    "        content = f.readlines()\n",
    "        \n",
    "        with codecs.open(out_path+file,'wb','utf8') as g:\n",
    "            g.write(title.strip()+'\\r\\n')\n",
    "            \n",
    "            second_line = []\n",
    "            \n",
    "            for line in content:\n",
    "                \n",
    "                try:\n",
    "                    line = line.strip()\n",
    "                    words = line.split()\n",
    "\n",
    "                    #check = False\n",
    "\n",
    "                    temp = []\n",
    "\n",
    "                    for word in words:\n",
    "                        if len(word.split('[')) > 1 and '+' in word.split('[')[1]:\n",
    "                            word = word.split('[')[0]\n",
    "                            #check = True\n",
    "                        if len(word.split('(')) != 2:\n",
    "                            #check = True\n",
    "                            continue\n",
    "                        \n",
    "                        temp.append(word)\n",
    "\n",
    "                    '''if check:\n",
    "                        print (line)\n",
    "                        print (' '.join(temp))'''\n",
    "                    \n",
    "                    second_line.append(' '.join(temp))\n",
    "\n",
    "                except:\n",
    "                    print ('ERROR',line)\n",
    "                    \n",
    "            g.write(' '.join(second_line)+'\\r\\n')\n",
    "                        \n",
    "    count = count + 1\n",
    "    p.value = count\n",
    "    p.description = str(count)\n",
    "        \n",
    "p.description = 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import time\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raw_txt\\\\\"\n",
    "out_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raw_txt2\\\\\"\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for file in os.listdir(file_path):\n",
    "    file_list.append(file)\n",
    "    \n",
    "for file in file_list:\n",
    "    title = ''\n",
    "    con = ''\n",
    "    check = False\n",
    "    with codecs.open(file_path+file,'rb','utf8') as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "        for i in content:\n",
    "            i = i.strip()\n",
    "            if len(i) == 0:\n",
    "                continue\n",
    "            elif i[0] == '<':\n",
    "                if 'DATE' in i and not check and len(con) == 0:\n",
    "                    title = (i.split('>')[1]).split('<')[0].strip()\n",
    "                    check = True\n",
    "            else:\n",
    "                con = con + i + '。'\n",
    "    with codecs.open(out_path+file,'wb','utf8') as g:\n",
    "        g.write(title+'\\r\\n')\n",
    "        g.write(con+'\\r\\n')\n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt\n",
      "10.txt\n",
      "eror 100.txt\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import time\n",
    "from ckip import CKIPSegmenter, CKIPParser\n",
    "\n",
    "segmenter = CKIPSegmenter('104753018', 'sayanouta')\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raw_txt3\\\\\"\n",
    "out_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raw_txt4\\\\\"\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for file in os.listdir(file_path):\n",
    "    file_list.append(file)\n",
    "    \n",
    "for file in file_list:\n",
    "    with codecs.open(file_path+file,'rb','utf8') as f:\n",
    "        title = f.readline().strip()\n",
    "        second = f.readline().strip()\n",
    "        \n",
    "        result = []\n",
    "        error_check = False\n",
    "        \n",
    "        '''for i in second.split('。'):\n",
    "            i = i + '。'\n",
    "            if i == '。':\n",
    "                continue\n",
    "            check = True\n",
    "            \n",
    "            while(check):\n",
    "                check = False\n",
    "                temper = []\n",
    "                try:\n",
    "                    segmented_result = segmenter.process(i)\n",
    "\n",
    "                    if segmented_result['status_code'] != '0':\n",
    "                        print ('Process Failed: ' + segmented_result['status'])\n",
    "                    else:\n",
    "                        for sentence in segmented_result['result']:\n",
    "                            for term in sentence:\n",
    "                                temper.append(term['term'] + '(' + term['pos'] + ')')\n",
    "                except:\n",
    "                    print ('*'+i)\n",
    "                    check = True\n",
    "                    \n",
    "                if not check:\n",
    "                    result += temper\n",
    "        print (file)\n",
    "        print (' '.join(result))'''\n",
    "        \n",
    "        try:\n",
    "            segmented_result = segmenter.process(second)\n",
    "\n",
    "            if segmented_result['status_code'] != '0':\n",
    "                print ('Process Failed: ' + segmented_result['status'])\n",
    "            else:\n",
    "                for sentence in segmented_result['result']:\n",
    "                    for term in sentence:\n",
    "                        result.append(term['term'] + '(' + term['pos'] + ')')\n",
    "        except:\n",
    "            print ('error',file)\n",
    "            error_check = True\n",
    "        \n",
    "        if error_check:    \n",
    "            with codecs.open(out_path+'error.txt','ab','utf8') as g:\n",
    "                g.write(file+'\\r\\n')\n",
    "        else:\n",
    "            with codecs.open(out_path+file,'wb','utf8') as g:\n",
    "                g.write(title+'\\r\\n')\n",
    "                g.write(' '.join(result)+'\\r\\n')\n",
    "            print (file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.173019\n",
      "0.0264351\n",
      "0.0129584\n",
      "0.0100924\n",
      "0.00895417\n",
      "0.00813672\n",
      "0.00749109\n",
      "0.00695715\n",
      "0.00654922\n",
      "0.00622826\n",
      "0.00597208\n",
      "0.00576422\n",
      "0.00559235\n",
      "0.00545113\n",
      "0.0053291\n",
      "0.00522076\n",
      "0.00511417\n",
      "0.00502088\n",
      "0.00493703\n",
      "0.0048573\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 添加层\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "# 1.训练的数据\n",
    "# Make up some real data \n",
    "x_data = np.linspace(-1,1,300)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.05, x_data.shape)\n",
    "y_data = np.square(x_data) - 0.5 + noise\n",
    "\n",
    "# 2.定义节点准备接收数据\n",
    "# define placeholder for inputs to network  \n",
    "xs = tf.placeholder(tf.float32, [None, 1])\n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# 3.定义神经层：隐藏层和预测层\n",
    "# add hidden layer 输入值是 xs，在隐藏层有 10 个神经元   \n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)\n",
    "# add output layer 输入值是隐藏层 l1，在预测层输出 1 个结果\n",
    "prediction = add_layer(l1, 10, 1, activation_function=None)\n",
    "\n",
    "# 4.定义 loss 表达式\n",
    "# the error between prediciton and real data    \n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),\n",
    "                     reduction_indices=[1]))\n",
    "\n",
    "# 5.选择 optimizer 使 loss 达到最小                   \n",
    "# 这一行定义了用什么方式去减少 loss，学习率是 0.1       \n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "\n",
    "# important step 对所有变量进行初始化\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "# 上面定义的都没有运算，直到 sess.run 才会开始运算\n",
    "sess.run(init)\n",
    "\n",
    "# 迭代 1000 次学习，sess.run optimizer\n",
    "for i in range(1000):\n",
    "    # training train_step 和 loss 都是由 placeholder 定义的运算，所以这里要用 feed 传入参数\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    if i % 50 == 0:\n",
    "        # to see the step improvement\n",
    "        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "胡適 ['01_All\\\\1.txt', '01_All\\\\3.txt', '02_All\\\\88.txt', '05_All\\\\705.txt', '05_All\\\\706.txt', '07_All\\\\1130.txt', '07_All\\\\1131.txt', '08_All\\\\1139.txt', '08_All\\\\1163.txt', '10_All\\\\1410.txt', '10_All\\\\1466.txt', '10_All\\\\1479.txt', '10_All\\\\1546.txt', '12_All\\\\1712.txt', '12_All\\\\1788.txt', '14_All\\\\2096.txt', '14_All\\\\2101.txt', '15_All\\\\2201.txt', '15_All\\\\2256.txt', '19_All\\\\2810.txt', '20_All\\\\3052.txt', '20_All\\\\3089.txt', '20_All\\\\3111.txt', '21_All\\\\3198.txt', '21_All\\\\3327.txt', '21_All\\\\3357.txt', '22_All\\\\3451.txt']\n",
      "朱伴耘 ['06_All\\\\939.txt', '07_All\\\\1050.txt', '07_All\\\\1073.txt', '07_All\\\\1118.txt', '08_All\\\\1154.txt', '08_All\\\\1221.txt', '08_All\\\\1231.txt', '08_All\\\\1253.txt', '08_All\\\\1263.txt', '09_All\\\\1328.txt', '09_All\\\\1385.txt', '09_All\\\\1402.txt', '10_All\\\\1412.txt', '10_All\\\\1449.txt', '10_All\\\\1491.txt', '10_All\\\\1539.txt', '11_All\\\\1613.txt', '12_All\\\\1713.txt', '14_All\\\\2004.txt', '16_All\\\\2395.txt', '17_All\\\\2521.txt', '17_All\\\\2547.txt', '17_All\\\\2569.txt', '18_All\\\\2674.txt', '18_All\\\\2740.txt', '19_All\\\\2855.txt', '20_All\\\\3157.txt', '21_All\\\\3254.txt', '23_All\\\\3632.txt']\n",
      "羅鴻詔 ['02_All\\\\118.txt', '02_All\\\\225.txt', '02_All\\\\74.txt', '03_All\\\\385.txt', '03_All\\\\404.txt', '04_All\\\\479.txt', '04_All\\\\525.txt', '04_All\\\\609.txt', '05_All\\\\641.txt', '05_All\\\\722.txt', '05_All\\\\778.txt', '06_All\\\\823.txt', '06_All\\\\893.txt', '07_All\\\\1000.txt', '07_All\\\\1011.txt', '07_All\\\\1036.txt', '08_All\\\\1140.txt', '08_All\\\\1197.txt', '09_All\\\\1295.txt', '09_All\\\\1318.txt', '09_All\\\\1361.txt', '10_All\\\\1502.txt', '12_All\\\\1740.txt', '12_All\\\\1847.txt', '13_All\\\\1982.txt', '13_All\\\\1995.txt']\n",
      "雷震 ['01_All\\\\7.txt', '02_All\\\\108.txt', '02_All\\\\155.txt', '02_All\\\\53.txt', '03_All\\\\262.txt', '03_All\\\\288.txt', '03_All\\\\294.txt', '03_All\\\\369.txt', '03_All\\\\396.txt', '04_All\\\\572.txt', '04_All\\\\586.txt', '05_All\\\\653.txt', '05_All\\\\710.txt', '05_All\\\\734.txt', '05_All\\\\777.txt', '05_All\\\\808.txt', '06_All\\\\838.txt', '06_All\\\\867.txt', '06_All\\\\881.txt', '06_All\\\\907.txt', '06_All\\\\922.txt', '07_All\\\\1095.txt', '07_All\\\\1102.txt', '07_All\\\\1108.txt', '07_All\\\\1120.txt', '08_All\\\\1143.txt', '08_All\\\\1157.txt', '08_All\\\\1166.txt', '08_All\\\\1243.txt', '08_All\\\\1255.txt', '09_All\\\\1316.txt', '11_All\\\\1672.txt', '12_All\\\\1730.txt', '13_All\\\\1873.txt', '13_All\\\\1889.txt', '13_All\\\\1972.txt', '14_All\\\\2129.txt', '15_All\\\\2264.txt', '15_All\\\\2272.txt', '16_All\\\\2385.txt', '18_All\\\\2641.txt', '18_All\\\\2758.txt', '19_All\\\\2940.txt', '20_All\\\\3033.txt', '20_All\\\\3075.txt', '20_All\\\\3096.txt', '20_All\\\\3116.txt', '20_All\\\\3136.txt', '20_All\\\\3152.txt', '20_All\\\\3163.txt', '20_All\\\\3178.txt', '20_All\\\\3189.txt', '21_All\\\\3338.txt', '22_All\\\\3436.txt', '22_All\\\\3465.txt', '22_All\\\\3474.txt', '22_All\\\\3491.txt', '22_All\\\\3520.txt', '23_All\\\\3618.txt', '23_All\\\\3643.txt']\n",
      "陳之藩 ['12_All\\\\1770.txt', '12_All\\\\1783.txt', '12_All\\\\1794.txt', '12_All\\\\1805.txt', '12_All\\\\1815.txt', '12_All\\\\1829.txt', '12_All\\\\1841.txt', '12_All\\\\1854.txt', '13_All\\\\1868.txt', '13_All\\\\1882.txt', '13_All\\\\1893.txt', '13_All\\\\1906.txt', '13_All\\\\1922.txt', '13_All\\\\1956.txt', '13_All\\\\1967.txt', '13_All\\\\1978.txt', '14_All\\\\2021.txt', '14_All\\\\2055.txt', '15_All\\\\2245.txt', '15_All\\\\2307.txt', '16_All\\\\2338.txt', '16_All\\\\2352.txt', '16_All\\\\2365.txt', '17_All\\\\2587.txt', '19_All\\\\2801.txt']\n",
      "徐訏 ['10_All\\\\1440.txt', '10_All\\\\1463.txt', '18_All\\\\2679.txt', '18_All\\\\2691.txt', '18_All\\\\2703.txt', '18_All\\\\2745.txt', '18_All\\\\2772.txt', '19_All\\\\2860.txt', '20_All\\\\3177.txt', '20_All\\\\3188.txt', '21_All\\\\3204.txt', '21_All\\\\3219.txt', '21_All\\\\3234.txt', '21_All\\\\3247.txt', '21_All\\\\3260.txt', '21_All\\\\3274.txt', '21_All\\\\3288.txt', '21_All\\\\3302.txt', '21_All\\\\3315.txt', '21_All\\\\3351.txt', '21_All\\\\3363.txt', '22_All\\\\3381.txt', '22_All\\\\3396.txt', '22_All\\\\3413.txt', '22_All\\\\3428.txt', '22_All\\\\3471.txt', '22_All\\\\3495.txt', '22_All\\\\3513.txt', '22_All\\\\3528.txt', '22_All\\\\3544.txt', '22_All\\\\3561.txt', '23_All\\\\3580.txt', '23_All\\\\3596.txt', '23_All\\\\3611.txt']\n",
      "蔣勻田 ['01_All\\\\25.txt', '02_All\\\\140.txt', '02_All\\\\146.txt', '02_All\\\\55.txt', '04_All\\\\623.txt', '07_All\\\\1107.txt', '08_All\\\\1220.txt', '08_All\\\\1262.txt', '09_All\\\\1273.txt', '10_All\\\\1432.txt', '10_All\\\\1468.txt', '11_All\\\\1602.txt', '11_All\\\\1673.txt', '12_All\\\\1789.txt', '14_All\\\\2062.txt', '14_All\\\\2127.txt', '14_All\\\\2140.txt', '15_All\\\\2150.txt', '15_All\\\\2163.txt', '15_All\\\\2190.txt', '15_All\\\\2250.txt', '15_All\\\\2266.txt', '15_All\\\\2287.txt', '16_All\\\\2316.txt', '16_All\\\\2411.txt', '16_All\\\\2437.txt', '17_All\\\\2492.txt', '17_All\\\\2584.txt', '20_All\\\\3183.txt', '21_All\\\\3339.txt']\n",
      "孟瑤 ['11_All\\\\1558.txt', '11_All\\\\1570.txt', '11_All\\\\1582.txt', '11_All\\\\1595.txt', '11_All\\\\1608.txt', '11_All\\\\1621.txt', '11_All\\\\1636.txt', '11_All\\\\1651.txt', '11_All\\\\1668.txt', '11_All\\\\1681.txt', '11_All\\\\1693.txt', '11_All\\\\1705.txt', '15_All\\\\2207.txt', '15_All\\\\2221.txt', '15_All\\\\2233.txt', '15_All\\\\2246.txt', '15_All\\\\2278.txt', '15_All\\\\2293.txt', '15_All\\\\2306.txt', '16_All\\\\2326.txt', '16_All\\\\2339.txt', '16_All\\\\2353.txt', '16_All\\\\2366.txt', '16_All\\\\2380.txt']\n",
      "龍平甫 ['07_All\\\\1039.txt', '07_All\\\\1089.txt', '08_All\\\\1141.txt', '08_All\\\\1164.txt', '08_All\\\\1199.txt', '08_All\\\\1225.txt', '08_All\\\\1236.txt', '08_All\\\\1256.txt', '09_All\\\\1287.txt', '09_All\\\\1298.txt', '09_All\\\\1332.txt', '09_All\\\\1365.txt', '10_All\\\\1436.txt', '10_All\\\\1471.txt', '10_All\\\\1515.txt', '11_All\\\\1556.txt', '11_All\\\\1568.txt', '11_All\\\\1603.txt', '11_All\\\\1643.txt', '11_All\\\\1690.txt', '12_All\\\\1731.txt', '12_All\\\\1769.txt', '12_All\\\\1793.txt', '13_All\\\\1863.txt', '13_All\\\\1915.txt', '13_All\\\\1953.txt', '14_All\\\\2018.txt', '14_All\\\\2039.txt', '14_All\\\\2090.txt', '15_All\\\\2165.txt', '15_All\\\\2204.txt', '15_All\\\\2289.txt', '15_All\\\\2302.txt', '16_All\\\\2398.txt', '16_All\\\\2460.txt', '17_All\\\\2496.txt', '17_All\\\\2525.txt', '17_All\\\\2615.txt', '18_All\\\\2677.txt', '19_All\\\\2799.txt', '19_All\\\\2871.txt']\n",
      "殷海光 ['01_All\\\\8.txt', '02_All\\\\191.txt', '02_All\\\\54.txt', '02_All\\\\89.txt', '03_All\\\\306.txt', '03_All\\\\392.txt', '03_All\\\\422.txt', '04_All\\\\455.txt', '04_All\\\\468.txt', '04_All\\\\485.txt', '04_All\\\\514.txt', '04_All\\\\568.txt', '04_All\\\\596.txt', '04_All\\\\630.txt', '05_All\\\\658.txt', '05_All\\\\688.txt', '05_All\\\\715.txt', '05_All\\\\740.txt', '05_All\\\\768.txt', '05_All\\\\801.txt', '06_All\\\\827.txt', '06_All\\\\872.txt', '06_All\\\\912.txt', '06_All\\\\989.txt', '07_All\\\\1014.txt', '07_All\\\\1042.txt', '07_All\\\\1100.txt', '08_All\\\\1193.txt', '08_All\\\\1205.txt', '08_All\\\\1251.txt', '08_All\\\\1264.txt', '10_All\\\\1422.txt', '10_All\\\\1475.txt', '10_All\\\\1480.txt', '10_All\\\\1536.txt', '10_All\\\\1541.txt', '12_All\\\\1801.txt', '13_All\\\\1902.txt', '13_All\\\\1916.txt', '14_All\\\\2081.txt', '14_All\\\\2095.txt', '17_All\\\\2582.txt', '18_All\\\\2724.txt', '18_All\\\\2754.txt', '19_All\\\\2813.txt', '19_All\\\\2935.txt', '20_All\\\\3113.txt', '20_All\\\\3147.txt', '22_All\\\\3517.txt', '22_All\\\\3521.txt', '22_All\\\\3554.txt', '23_All\\\\3590.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\碩士班\\\\SNA\\\\meeting\\\\source\\\\自由中國\\\\\"\n",
    "\n",
    "name = ['孟瑤', '徐訏', '朱伴耘', '殷海光', '羅鴻詔', '胡適', '蔣勻田', '陳之藩', '雷震', '龍平甫']\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for d in os.listdir(path):\n",
    "    for file in os.listdir(path+d+'\\\\'):\n",
    "        file_list.append(d+'\\\\'+file)\n",
    "        \n",
    "frequency = defaultdict(list)\n",
    "\n",
    "for file in file_list:\n",
    "    with codecs.open(path+file,'rb','utf8') as f:\n",
    "        header = (f.readline().strip()).split()\n",
    "        \n",
    "        if len(header) >= 6 and header[5] in name:\n",
    "            frequency[header[5]].append(file)\n",
    "            \n",
    "for item in frequency:\n",
    "    print (item,frequency[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\\"\n",
    "\n",
    "loc_index = []\n",
    "\n",
    "with codecs.open(path+'地名2.csv','rb','utf8') as f:\n",
    "    header = f.readline()\n",
    "    content = f.readlines()\n",
    "    \n",
    "    for line in content:\n",
    "        for words in (line.strip()).split(','):\n",
    "            if len(words) != 0:\n",
    "                for word in words.split('、'):\n",
    "                    if word not in loc_index and len(word) != 0 and word != ' ':\n",
    "                        loc_index.append(word)\n",
    "                        \n",
    "    \n",
    "'''with codecs.open(path+'地名.txt','wb','utf8') as g:\n",
    "    for word in loc_index:\n",
    "        g.write(word+'\\r\\n')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-12 11:01:20,946 : INFO : collecting all words and their counts\n",
      "2016-12-12 11:01:20,950 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-12-12 11:01:21,401 : INFO : collected 49042 word types from a corpus of 935972 raw words and 94 sentences\n",
      "2016-12-12 11:01:21,401 : INFO : Loading a fresh vocabulary\n",
      "2016-12-12 11:01:21,477 : INFO : min_count=5 retains 12558 unique words (25% of original 49042, drops 36484)\n",
      "2016-12-12 11:01:21,478 : INFO : min_count=5 leaves 878915 word corpus (93% of original 935972, drops 57057)\n",
      "2016-12-12 11:01:21,546 : INFO : deleting the raw counts dictionary of 49042 items\n",
      "2016-12-12 11:01:21,546 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2016-12-12 11:01:21,546 : INFO : downsampling leaves estimated 775820 word corpus (88.3% of prior 878915)\n",
      "2016-12-12 11:01:21,559 : INFO : estimated required memory for 12558 words and 1000 dimensions: 106743000 bytes\n",
      "2016-12-12 11:01:21,629 : INFO : resetting layer weights\n",
      "2016-12-12 11:01:22,103 : INFO : training model with 3 workers on 12558 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2016-12-12 11:01:22,103 : INFO : expecting 94 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-12 11:01:23,123 : INFO : PROGRESS: at 4.47% examples, 175222 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-12 11:01:24,120 : INFO : PROGRESS: at 9.36% examples, 182281 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:25,136 : INFO : PROGRESS: at 14.68% examples, 189265 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:26,140 : INFO : PROGRESS: at 19.57% examples, 189359 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-12 11:01:27,197 : INFO : PROGRESS: at 24.04% examples, 184228 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:28,213 : INFO : PROGRESS: at 29.15% examples, 186055 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:29,283 : INFO : PROGRESS: at 34.47% examples, 186897 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:30,368 : INFO : PROGRESS: at 39.57% examples, 185984 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:31,470 : INFO : PROGRESS: at 44.26% examples, 183865 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:32,478 : INFO : PROGRESS: at 48.09% examples, 180328 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:33,519 : INFO : PROGRESS: at 53.40% examples, 181748 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:34,559 : INFO : PROGRESS: at 58.51% examples, 182774 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-12 11:01:35,558 : INFO : PROGRESS: at 63.19% examples, 182417 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:36,587 : INFO : PROGRESS: at 68.30% examples, 183327 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:37,589 : INFO : PROGRESS: at 72.34% examples, 181529 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-12 11:01:38,649 : INFO : PROGRESS: at 77.66% examples, 182393 words/s, in_qsize 6, out_qsize 0\n",
      "2016-12-12 11:01:39,672 : INFO : PROGRESS: at 82.77% examples, 182984 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:40,696 : INFO : PROGRESS: at 87.87% examples, 183622 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:41,705 : INFO : PROGRESS: at 93.19% examples, 184630 words/s, in_qsize 4, out_qsize 0\n",
      "2016-12-12 11:01:42,739 : INFO : PROGRESS: at 98.09% examples, 184621 words/s, in_qsize 5, out_qsize 0\n",
      "2016-12-12 11:01:43,022 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-12 11:01:43,045 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-12 11:01:43,092 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-12 11:01:43,093 : INFO : training on 4679860 raw words (3879356 effective words) took 21.0s, 184918 effective words/s\n",
      "2016-12-12 11:01:43,115 : INFO : storing 12558x1000 projection weights into C:\\Users\\user\\Desktop\\diary-1000.model.bin\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\\"\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.Text8Corpus(path+\"diary.txt\")  # 加载语料\n",
    "#model2 = word2vec.Word2Vec.load_word2vec_format(path+\"diary.model.bin\", binary=True)\n",
    "model = word2vec.Word2Vec(sentences, size=1000)  # 训练skip-gram模型; 默认window=5\n",
    "model.save_word2vec_format(path+\"diary-1000.model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-12 11:01:47,552 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "和[臺北]最相關的詞有：\n",
      "\n",
      "西門町 0.919585645198822\n",
      "齊世英處 0.9039710164070129\n",
      "機場 0.8941402435302734\n",
      "臺大 0.8923606872558594\n",
      "基隆 0.8858584761619568\n",
      "中山室 0.8791376948356628\n",
      "傅正處 0.8771236538887024\n",
      "松江路 0.8697463870048523\n",
      "埤腹 0.8638896346092224\n",
      "紹唐處 0.8614902496337891\n",
      "醫院 0.8496037125587463\n",
      "臺中 0.8458173871040344\n",
      "圓通寺 0.8420592546463013\n",
      "中心 0.8394351601600647\n",
      "回話 0.8386403918266296\n",
      "草山 0.8385666012763977\n",
      "雪艇處 0.8377959728240967\n",
      "南港 0.8351216912269592\n",
      "勻田處 0.8344115018844604\n",
      "巴士 0.8324846029281616\n",
      "景美 0.8302963376045227\n",
      "十一時 0.8297519087791443\n",
      "北投 0.8296608328819275\n",
      "木柵 0.8278195858001709\n",
      "深夜 0.8275622129440308\n",
      "晤 0.8250057101249695\n",
      "滬 0.8248897194862366\n",
      "社 0.8212802410125732\n",
      "飛機 0.8208167552947998\n",
      "墓 0.8202067017555237\n",
      "0.54171244786\n"
     ]
    }
   ],
   "source": [
    "find = '臺北'\n",
    "t = model.most_similar(find,topn=30)\n",
    "print ('和['+find+']最相關的詞有：\\n')\n",
    "for item in t:\n",
    "    print (item[0],item[1])\n",
    "t3 = model.similarity('臺北','英雄館')\n",
    "print (t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5396\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "loc_frequency = defaultdict(int)\n",
    "\n",
    "loc_index = []\n",
    "\n",
    "with codecs.open(path+'地名.txt','rb','utf8') as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "    for i in content:\n",
    "        loc_index.append(i.strip())\n",
    "\n",
    "for add in loc_index:\n",
    "    try:\n",
    "        t = model.most_similar(add,topn=100)\n",
    "        loc_frequency[add] += 1\n",
    "        for item in t:\n",
    "            loc_frequency[item[0]] += 1\n",
    "    except:\n",
    "        loc_frequency[add] += 1\n",
    "\n",
    "print (len(loc_frequency))\n",
    "\n",
    "sort_words = sorted(loc_frequency.items(), key=lambda d:d[1], reverse = True)\n",
    "\n",
    "with codecs.open(path+'地名2.txt','wb','utf8') as g:\n",
    "    for word in sort_words:\n",
    "        g.write(word[0]+' '+str(word[1])+'\\r\\n')\n",
    "        #g.write(word[0]+'\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "path1 = \"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\自由中國母體\\\\\"\n",
    "path2 = \"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\自由中國母體(無文藝類)\\\\\"\n",
    "out_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\自由中國(文藝類)\\\\'\n",
    "\n",
    "file_list = []\n",
    "\n",
    "head = {}\n",
    "\n",
    "for file in os.listdir(path2):\n",
    "    file_list.append(file)\n",
    "    \n",
    "for file in file_list:\n",
    "    with codecs.open(path2+file,'rb','utf8') as f:\n",
    "        header = f.readline()\n",
    "        if header not in head:\n",
    "            head[header] = file\n",
    "        else:\n",
    "            print ('ERROR,',file,head[header])\n",
    "            \n",
    "file_list2 = []\n",
    "con_file = []\n",
    "\n",
    "for file in os.listdir(path1):\n",
    "    file_list2.append(file)\n",
    "    \n",
    "for file in file_list2:\n",
    "    with codecs.open(path1+file,'rb','utf8') as f: \n",
    "        header = f.readline()\n",
    "        \n",
    "        if header not in head:\n",
    "            con_file.append(file)\n",
    "count = 1\n",
    "for file in con_file:\n",
    "    with codecs.open(path1+file,'rb','utf8') as f: \n",
    "        header = f.readline()\n",
    "        content = f.readline()\n",
    "        \n",
    "        with codecs.open(out_path+str(count)+'.txt','wb','utf8') as g: \n",
    "            g.write(header.strip()+'\\r\\n')\n",
    "            g.write(content.strip()+'\\r\\n')\n",
    "            \n",
    "        count += 1\n",
    "        \n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "孟瑤.txt 61713\n",
      "徐訏.txt 78889\n",
      "朱伴耘.txt 86752\n",
      "殷海光.txt 127123\n",
      "羅鴻詔.txt 60487\n",
      "胡適.txt 32396\n",
      "蔣勻田.txt 65521\n",
      "陳之藩.txt 12653\n",
      "雷震.txt 173501\n",
      "龍平甫.txt 133318\n",
      "83235.3\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\作者\\\\\"\n",
    "\n",
    "all_words = 0\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    with codecs.open(path+file,'rb','utf8') as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "        word_count = 0\n",
    "        \n",
    "        for i in content:\n",
    "            word_count += len((i.strip()).split())\n",
    "        print (file,word_count)\n",
    "        all_words += word_count\n",
    "print (all_words/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15511603 10192572 3632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in aart:\\n    if len(aart[i]) != 1:\\n        print (i,aart[i])'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "from collections import defaultdict \n",
    "\n",
    "#path = \"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\自由中國(文藝類)\\\\\"\n",
    "#path = \"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\碩士班\\\\SNA\\\\meeting\\\\source\\\\自由中國\\\\\"\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\自由中國\\\\自由中國-全\\\\\"\n",
    "\n",
    "out_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\自由中國(非文藝類)\\\\\"\n",
    "\n",
    "'''art = []\n",
    "aart = defaultdict(dict)\n",
    "with codecs.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\文藝.txt\",'rb','utf8') as f:\n",
    "    content = f.readlines()\n",
    "    for i in content:\n",
    "        art.append(i.strip())\n",
    "        aart[i.strip()] = []'''\n",
    "\n",
    "t = 0\n",
    "word = 0\n",
    "counts = 0\n",
    "\n",
    "'''for d in os.listdir(path):\n",
    "    for file in os.listdir(path+d+'\\\\'):\n",
    "        with codecs.open(path+d+'\\\\'+file,'rb','utf8') as f:\n",
    "            head = (f.readline()).strip().split()\n",
    "            content = f.readline().strip().split()\n",
    "            \n",
    "            if len(head) >= 6 and head[4] in art:\n",
    "                counts += len(content)\n",
    "                aart[head[4]].append(d+'_'+file)\n",
    "\n",
    "                for i in [i.split('(')[0] for i in content if len(i.split('(')) == 2 and i.split('(')[0] != '']:\n",
    "                    word += len(i)'''\n",
    "\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    with codecs.open(path+file,'rb','utf8') as f:\n",
    "        t += 1\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "\n",
    "        counts += len(content)\n",
    "\n",
    "        for i in [i.split('(')[0] for i in content if len(i.split('(')) == 2 and i.split('(')[0] != '']:\n",
    "            word += len(i)\n",
    "print (word,counts,t)\n",
    "'''for i in aart:\n",
    "    if len(aart[i]) != 1:\n",
    "        print (i,aart[i])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "from collections import defaultdict \n",
    "\n",
    "path = \"C:\\\\Users\\\\user\\\\Desktop\\\\自由中國\\\\自由中國-全\\\\\"\n",
    "#path = \"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\文章+社論\\\\\"\n",
    "out_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\自由中國\\\\自由中國-非文藝類\\\\\"\n",
    "\n",
    "art = []\n",
    "check = defaultdict(list)\n",
    "\n",
    "with codecs.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\文藝.txt\",'rb','utf8') as f:\n",
    "    content = f.readlines()\n",
    "    for i in content:\n",
    "        art.append(i.strip())\n",
    "        #check[i.strip()] = []\n",
    "\n",
    "'''for d in os.listdir(path):\n",
    "    for file in os.listdir(path+d+'\\\\'):\n",
    "        with codecs.open(path+d+'\\\\'+file,'rb','utf8') as f:\n",
    "            head = (f.readline()).strip()\n",
    "            content = (f.readline()).strip()\n",
    "            check[head].append(d+'\\\\'+file)\n",
    "            if len(content) == 0 or len(head) == 0:\n",
    "                print (d,file,head)'''\n",
    "                \n",
    "for file in os.listdir(path):\n",
    "    with codecs.open(path+file,'rb','utf8') as f: \n",
    "        head = (f.readline()).strip().split()\n",
    "        content = (f.readline()).strip()\n",
    "        if len(head) >= 5 and head[4] not in art:\n",
    "            check[' '.join(head)].append(file)\n",
    "            \n",
    "            \n",
    "for i in check:\n",
    "    if len(check[i]) != 1:\n",
    "        print (i,check[i])\n",
    "\n",
    "for i in check:\n",
    "    for j in check[i]:\n",
    "        with codecs.open(path+j,'rb','utf8') as f:\n",
    "            content = f.read()\n",
    "            with codecs.open(out_path+j,'wb','utf8') as g:\n",
    "                g.write(content)\n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "\n",
    "path1 = \"C:\\\\Users\\\\user\\\\Desktop\\\\自由中國\\\\自由中國-非文藝類\\\\\"\n",
    "path2 = \"C:\\\\Users\\\\user\\\\Desktop\\\\自由中國\\\\自由中國-雷震文章+社論\\\\\"\n",
    "out_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\自由中國\\\\自由中國-非文藝類非雷震\\\\\"\n",
    "\n",
    "condicate = []\n",
    "\n",
    "for file in os.listdir(path2):\n",
    "    with codecs.open(path2+file,'rb','utf8') as f:\n",
    "        head = f.readline().strip()\n",
    "        condicate.append(head)\n",
    "\n",
    "for file in os.listdir(path1):\n",
    "    with codecs.open(path1+file,'rb','utf8') as f:\n",
    "        head = f.readline().strip()\n",
    "        content = f.readline().strip()\n",
    "        if head not in condicate:\n",
    "            with codecs.open(out_path+file,'wb','utf8') as g:\n",
    "                g.write(head+'\\r\\n')\n",
    "                g.write(content+'\\r\\n')\n",
    "\n",
    "print ('END')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
