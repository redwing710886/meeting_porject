{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for file in os.listdir(SC_path):\\n    with codecs.open(SC_path+file,'rb','utf8') as f:\\n        head = f.readline()\\n        content = f.readline().strip().split()\\n        SC_num += len(content)\\n        \\nfor file in os.listdir(FC_path):\\n    with codecs.open(FC_path+file,'rb','utf8') as f:\\n        head = f.readline()\\n        content = f.readline().strip().split()\\n        FC_num += len(content)\\n        \\nfor file in os.listdir(lei_path):\\n    with codecs.open(lei_path+file,'rb','utf8') as f:\\n        head = f.readline()\\n        content = f.readline().strip().split()\\n        \\n        if '日記' in file:\\n            lei_diary += len(content)        \\n        elif '文章' in file:\\n            lei_article += len(content)\\n        elif '社論' in file:\\n            lei_social += len(content)\\n            \\nprint ('總詞數')\\nprint ('平衡語料庫：',SC_num)\\nprint ('自由中國：',FC_num)\\nprint ('雷震日記：',lei_diary)\\nprint ('雷震社論：',lei_social)\\nprint ('雷震文章：',lei_article)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#找出各母體詞頻\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "SC_num = 0\n",
    "FC_num = 0\n",
    "lei_diary = 0\n",
    "lei_social = 0\n",
    "lei_article = 0\n",
    "\n",
    "SC_path = 'D:\\\\課業相關\\\\論文資料\\\\SCS2\\\\'\n",
    "FC_path = 'D:\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\自由中國(2nd)\\\\自由中國-非文藝類\\\\'\n",
    "lei_path = 'D:\\課業相關\\\\論文資料\\\\論文程式\\\\condicate\\\\topic\\\\'\n",
    "\n",
    "'''for file in os.listdir(SC_path):\n",
    "    with codecs.open(SC_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        SC_num += len(content)\n",
    "        \n",
    "for file in os.listdir(FC_path):\n",
    "    with codecs.open(FC_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        FC_num += len(content)\n",
    "        \n",
    "for file in os.listdir(lei_path):\n",
    "    with codecs.open(lei_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        \n",
    "        if '日記' in file:\n",
    "            lei_diary += len(content)        \n",
    "        elif '文章' in file:\n",
    "            lei_article += len(content)\n",
    "        elif '社論' in file:\n",
    "            lei_social += len(content)\n",
    "            \n",
    "print ('總詞數')\n",
    "print ('平衡語料庫：',SC_num)\n",
    "print ('自由中國：',FC_num)\n",
    "print ('雷震日記：',lei_diary)\n",
    "print ('雷震社論：',lei_social)\n",
    "print ('雷震文章：',lei_article)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"all_feature_num = []\\nfor i in vector_base:\\n    temp_path = feature_path\\n    base_sum = 0\\n    if i == 'SC':\\n        temp_path += '平衡語料庫\\\\'\\n        base_sum = SC_num\\n    elif i == 'FC':\\n        temp_path += '自由中國\\\\'\\n        base_sum = FC_num\\n    elif i == 'lei':\\n        temp_path += '雷震文本\\\\'\\n        base_sum = lei_diary+lei_article+lei_social\\n        \\n    file_list = []\\n    for j in feature_condicate:\\n        for file in os.listdir(temp_path):\\n            if j in file:\\n                file_list.append(file)\\n                break\\n    \\n    for index,j in enumerate(file_list):\\n        feature_num = [] \\n        with codecs.open(temp_path+j,'rb','utf8') as f: #抓取基準特徵\\n            for k in f.readlines():\\n                if k.strip() != '':\\n                    feature_num.append(str(round(int(k.strip().split(',')[-1])*1000000/base_sum)))\\n        if len(feature_num) != 100 and '標點' not in j:\\n            print (j,'ERROR')\\n        else:\\n            feature_num.insert(0,i+' '+feature_condicate[index])\\n            all_feature_num.append(feature_num)\\n            \\nwith codecs.open(desktop_path+'feature_base_vector.csv','wb','utf8') as g:\\n    for i in all_feature_num:\\n        g.write(','.join(i)+'\\r\\n')\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#找出各母體語言特徵向量相對頻率\n",
    "import time\n",
    "\n",
    "desktop_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\'\n",
    "\n",
    "feature_path = 'D:\\\\課業相關\\\\論文資料\\\\論文程式\\\\language_feature\\\\最終版\\\\'\n",
    "\n",
    "vector_base = ['SC','FC','lei']\n",
    "feature_condicate = ['高頻','bigram','trigram','標點','N+N','N+V','VH+N','D+V','否定','程度','情態']\n",
    "\n",
    "'''all_feature_num = []\n",
    "for i in vector_base:\n",
    "    temp_path = feature_path\n",
    "    base_sum = 0\n",
    "    if i == 'SC':\n",
    "        temp_path += '平衡語料庫\\\\'\n",
    "        base_sum = SC_num\n",
    "    elif i == 'FC':\n",
    "        temp_path += '自由中國\\\\'\n",
    "        base_sum = FC_num\n",
    "    elif i == 'lei':\n",
    "        temp_path += '雷震文本\\\\'\n",
    "        base_sum = lei_diary+lei_article+lei_social\n",
    "        \n",
    "    file_list = []\n",
    "    for j in feature_condicate:\n",
    "        for file in os.listdir(temp_path):\n",
    "            if j in file:\n",
    "                file_list.append(file)\n",
    "                break\n",
    "    \n",
    "    for index,j in enumerate(file_list):\n",
    "        feature_num = [] \n",
    "        with codecs.open(temp_path+j,'rb','utf8') as f: #抓取基準特徵\n",
    "            for k in f.readlines():\n",
    "                if k.strip() != '':\n",
    "                    feature_num.append(str(round(int(k.strip().split(',')[-1])*1000000/base_sum)))\n",
    "        if len(feature_num) != 100 and '標點' not in j:\n",
    "            print (j,'ERROR')\n",
    "        else:\n",
    "            feature_num.insert(0,i+' '+feature_condicate[index])\n",
    "            all_feature_num.append(feature_num)\n",
    "            \n",
    "with codecs.open(desktop_path+'feature_base_vector.csv','wb','utf8') as g:\n",
    "    for i in all_feature_num:\n",
    "        g.write(','.join(i)+'\\r\\n')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_vector = 'C:\\\\Users\\\\user\\\\Desktop\\\\RF result\\\\vector\\\\'\n",
    "\n",
    "base_value = defaultdict(list)\n",
    "with codecs.open(desktop_path+'feature_base_vector.csv','rb','utf8') as f:\n",
    "    for line in f.readlines(): \n",
    "        if line.strip() == '':\n",
    "            continue\n",
    "        base_value[line.strip().split(',')[0]] = list(map(int,map(float,line.strip().split(',')[1:])))\n",
    "        \n",
    "for vb in vector_base:\n",
    "    for fc in feature_condicate:\n",
    "        for cs in ['name','topic']:\n",
    "            \n",
    "            if (vb == 'FC' and cs == 'topic') or (vb == 'lei' and cs == 'name'):\n",
    "                continue\n",
    "      \n",
    "            classification_num = defaultdict(dict)\n",
    "\n",
    "            with codecs.open(test_vector+vb+' '+fc+' '+cs+'.csv','rb','utf8') as f:\n",
    "                for i in f.readlines():\n",
    "                    if i.strip() == '':\n",
    "                        continue\n",
    "                    line = i.strip().split(',')\n",
    "                    for index,j in enumerate(line[1:]):\n",
    "                        if str(index) not in classification_num[line[0]]:\n",
    "                            classification_num[line[0]][str(index)] = [j]\n",
    "                        else:\n",
    "                            classification_num[line[0]][str(index)].append(j)\n",
    "\n",
    "            for k,v in classification_num.items():\n",
    "                #print (k)\n",
    "                temp_rsd = []\n",
    "                for i in range(len(v)):\n",
    "                    temp_num = list(map(int,map(float,v[str(i)])))\n",
    "\n",
    "                    feature_base_value = base_value[vb+' '+fc][i]\n",
    "                    if feature_base_value == 0:\n",
    "                        break\n",
    "\n",
    "                    '''p_rsd = [(x-feature_base_value)**2 for x in temp_num if x >= feature_base_value]\n",
    "                    n_rsd = [(feature_base_value-x)**2 for x in temp_num if x < feature_base_value]\n",
    "\n",
    "                    if len(p_rsd) != 0:\n",
    "                        p_rsd = (sum(p_rsd)/len(p_rsd))**0.5/feature_base_value #這邊採用N而非N-1作為標準差分母\n",
    "                    else:\n",
    "                        p_rsd = 0\n",
    "\n",
    "                    if len(n_rsd) != 0:\n",
    "                        n_rsd = (sum(n_rsd)/len(n_rsd))**0.5/feature_base_value\n",
    "                    else:\n",
    "                        n_rsd = 0\n",
    "                        \n",
    "                    temp_rsd.append(str(round(p_rsd-n_rsd,5))) #RSD 相對標準差'''\n",
    "                    \n",
    "                    '''answer = 0\n",
    "                    if p_rsd-n_rsd > 0:\n",
    "                        answer = int((p_rsd-n_rsd)/(0.33)) + 1\n",
    "                    elif p_rsd-n_rsd < 0:\n",
    "                        answer = int((p_rsd-n_rsd)/(1/3)) - 1'''\n",
    "                    \n",
    "                    #print (p_rsd-n_rsd,answer)\n",
    "                    #time.sleep(0.3)\n",
    "                    #temp_rsd.append(str(answer))\n",
    "                    \n",
    "                    '''rsd = [(x-feature_base_value)**2 for x in temp_num]\n",
    "                    rsd = (sum(rsd)/len(rsd))**0.5/feature_base_value\n",
    "                    #if rsd <= 1:\n",
    "                    #    rsd = 0\n",
    "                    temp_rsd.append(str(round(rsd,5)))'''\n",
    "                    \n",
    "                    avg = sum(temp_num)/len(temp_num)\n",
    "                    rsd = [(x-avg)**2 for x in temp_num]\n",
    "                    if avg != 0:\n",
    "                        rsd = (sum(rsd)/len(rsd))**0.5/avg\n",
    "                    else:\n",
    "                        rsd = 0\n",
    "                    temp_rsd.append(str(round(rsd,5)))\n",
    "                    \n",
    "                    \n",
    "                with codecs.open(desktop_path+'rsd_feature2.csv','ab','utf8') as a:\n",
    "                    a.write(vb+' '+fc+' '+cs+' '+k+','+','.join(temp_rsd)+'\\r\\n')\n",
    "                '''print (vb+' '+fc+' '+cs+' '+k)\n",
    "                print (temp_rsd)\n",
    "                time.sleep(0.3)'''\n",
    "print ('END')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
