{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for file in os.listdir(SC_path):\\n    with codecs.open(SC_path+file,'rb','utf8') as f:\\n        head = f.readline()\\n        content = f.readline().strip().split()\\n        SC_num += len(content)\\n        \\nfor file in os.listdir(FC_path):\\n    with codecs.open(FC_path+file,'rb','utf8') as f:\\n        head = f.readline()\\n        content = f.readline().strip().split()\\n        FC_num += len(content)\\n        \\nfor file in os.listdir(lei_path):\\n    with codecs.open(lei_path+file,'rb','utf8') as f:\\n        head = f.readline()\\n        content = f.readline().strip().split()\\n        \\n        if '日記' in file:\\n            lei_diary += len(content)        \\n        elif '文章' in file:\\n            lei_article += len(content)\\n        elif '社論' in file:\\n            lei_social += len(content)\\n            \\nprint ('總詞數')\\nprint ('平衡語料庫：',SC_num)\\nprint ('自由中國：',FC_num)\\nprint ('雷震日記：',lei_diary)\\nprint ('雷震社論：',lei_social)\\nprint ('雷震文章：',lei_article)\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#找出各母體詞頻\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "SC_num = 0\n",
    "FC_num = 0\n",
    "lei_diary = 0\n",
    "lei_social = 0\n",
    "lei_article = 0\n",
    "\n",
    "SC_path = 'D:\\\\課業相關\\\\論文資料\\\\SCS2\\\\'\n",
    "FC_path = 'D:\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\自由中國(2nd)\\\\自由中國-非文藝類\\\\'\n",
    "lei_path = 'D:\\課業相關\\\\論文資料\\\\論文程式\\\\condicate\\\\topic\\\\'\n",
    "\n",
    "'''for file in os.listdir(SC_path):\n",
    "    with codecs.open(SC_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        SC_num += len(content)\n",
    "        \n",
    "for file in os.listdir(FC_path):\n",
    "    with codecs.open(FC_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        FC_num += len(content)\n",
    "        \n",
    "for file in os.listdir(lei_path):\n",
    "    with codecs.open(lei_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        \n",
    "        if '日記' in file:\n",
    "            lei_diary += len(content)        \n",
    "        elif '文章' in file:\n",
    "            lei_article += len(content)\n",
    "        elif '社論' in file:\n",
    "            lei_social += len(content)\n",
    "            \n",
    "print ('總詞數')\n",
    "print ('平衡語料庫：',SC_num)\n",
    "print ('自由中國：',FC_num)\n",
    "print ('雷震日記：',lei_diary)\n",
    "print ('雷震社論：',lei_social)\n",
    "print ('雷震文章：',lei_article)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "#找出各母體語言特徵向量相對頻率\n",
    "import time\n",
    "\n",
    "desktop_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\'\n",
    "\n",
    "feature_path = 'D:\\\\課業相關\\\\論文資料\\\\論文程式\\\\language_feature\\\\最終版\\\\'\n",
    "\n",
    "vector_base = ['SC','FC','lei']\n",
    "feature_condicate = ['高頻','bigram','trigram','標點','N+N','N+V','VH+N','D+V','否定','程度','情態']\n",
    "\n",
    "all_feature_num = []\n",
    "all_feature = {}\n",
    "for i in vector_base:\n",
    "    temp_path = feature_path\n",
    "    base_sum = 0\n",
    "    if i == 'SC':\n",
    "        temp_path += '平衡語料庫\\\\'\n",
    "        base_sum = SC_num\n",
    "    elif i == 'FC':\n",
    "        temp_path += '自由中國\\\\'\n",
    "        base_sum = FC_num\n",
    "    elif i == 'lei':\n",
    "        temp_path += '雷震文本\\\\'\n",
    "        base_sum = lei_diary+lei_article+lei_social\n",
    "        \n",
    "    file_list = []\n",
    "    for j in feature_condicate:\n",
    "        for file in os.listdir(temp_path):\n",
    "            if j in file:\n",
    "                file_list.append(file)\n",
    "                break\n",
    "    \n",
    "    '''for index,j in enumerate(file_list):\n",
    "        feature_num = [] \n",
    "        with codecs.open(temp_path+j,'rb','utf8') as f: #抓取基準特徵\n",
    "            for k in f.readlines():\n",
    "                if k.strip() != '':\n",
    "                    feature_num.append(str(round(int(k.strip().split(',')[-1])*1000000/base_sum)))\n",
    "        if len(feature_num) != 100 and '標點' not in j:\n",
    "            print (j,'ERROR')\n",
    "        else:\n",
    "            feature_num.insert(0,i+' '+feature_condicate[index])\n",
    "            all_feature_num.append(feature_num)'''\n",
    "    \n",
    "    for index,j in enumerate(file_list):\n",
    "        feature_list = [] \n",
    "        with codecs.open(temp_path+j,'rb','utf8') as f: #抓取基準特徵\n",
    "            for k in f.readlines():\n",
    "                if k.strip() != '':\n",
    "                    if len(k.strip().split(',')) == 2:\n",
    "                        if '\\ufeff' in k: #去掉開頭BOM\n",
    "                            k = k.replace('\\ufeff','')\n",
    "                        feature_list.append(k.strip().split(',')[0])\n",
    "                    else:\n",
    "                        feature_list.append('')\n",
    "        if len(feature_list) != 100 and '標點' not in j:\n",
    "            print (j,'ERROR')\n",
    "        else:\n",
    "            #feature_list.insert(0,i+' '+feature_condicate[index])\n",
    "            all_feature[i+' '+feature_condicate[index]] = feature_list\n",
    "            \n",
    "'''with codecs.open(desktop_path+'feature_base_vector.csv','wb','utf8') as g:\n",
    "    for i in all_feature_num:\n",
    "        g.write(','.join(i)+'\\r\\n')'''\n",
    "\n",
    "'''with codecs.open(desktop_path+'feature_base_list.csv','wb','utf8') as g:\n",
    "    for i in vector_base:\n",
    "        for j in feature_condicate:\n",
    "            g.write(i+' '+j+','+','.join(all_feature[i+' '+j])+'\\r\\n')'''\n",
    "        \n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  918.29213483 -1714.34831461]\n",
      " [ 3315.025        586.6875    ]\n",
      " [ 4724.9047619   -304.11111111]\n",
      " [ -600.8125      1252.97916667]\n",
      " [-4554.70731707  2056.14634146]\n",
      " [-4938.63333333 -1333.93333333]\n",
      " [ 5426.31034483   639.34482759]\n",
      " [ 2861.7037037   1239.33333333]\n",
      " [ 6058.73076923 -4667.76923077]]\n",
      "[[ 0.53257539  0.43924123]\n",
      " [ 0.75051242  0.78145787]\n",
      " [ 0.87871403  0.64897575]\n",
      " [ 0.39444187  0.88055068]\n",
      " [ 0.03491073  1.        ]\n",
      " [ 0.          0.49581763]\n",
      " [ 0.94249345  0.78928922]\n",
      " [ 0.70929151  0.87852123]\n",
      " [ 1.          0.        ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1604bf45a952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     a.write(vb+' '+fc+' '+cs+' '+str(i)+','+','.join(list(map(str,list(vector_space[i]))))+'\\r\\n')'''\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'END'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#比較各類別分布表現\n",
    "import codecs\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "desktop_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\'\n",
    "\n",
    "feature_path = 'D:\\\\課業相關\\\\論文資料\\\\論文程式\\\\language_feature\\\\最終版\\\\'\n",
    "\n",
    "vector_base = ['SC','FC','lei']\n",
    "feature_condicate = ['高頻','bigram','trigram','標點','N+N','N+V','VH+N','D+V','否定','程度','情態']\n",
    "\n",
    "test_vector = 'C:\\\\Users\\\\user\\\\Desktop\\\\RF result\\\\vector\\\\'\n",
    "\n",
    "all_vector_space = {}\n",
    "\n",
    "base_value = defaultdict(list)\n",
    "with codecs.open(desktop_path+'feature_base_vector.csv','rb','utf8') as f:\n",
    "    for line in f.readlines(): \n",
    "        if line.strip() == '':\n",
    "            continue\n",
    "        base_value[line.strip().split(',')[0]] = list(map(int,map(float,line.strip().split(',')[1:])))\n",
    "\n",
    "#各類別文本平均值與母體平均之差異\n",
    "for vb in vector_base:\n",
    "    for fc in feature_condicate:\n",
    "        for cs in ['name','topic']:\n",
    "            \n",
    "            if (vb == 'FC' and cs == 'topic') or (vb == 'lei' and cs == 'name'):\n",
    "                continue\n",
    "      \n",
    "            classification_num = defaultdict(dict) #類別-詞組-文本數值\n",
    "        \n",
    "            feature_num = 0\n",
    "\n",
    "            with codecs.open(test_vector+vb+' '+fc+' '+cs+'.csv','rb','utf8') as f: #將各類別的詞組整合成同一陣列\n",
    "                for i in f.readlines():\n",
    "                    if i.strip() == '':\n",
    "                        continue\n",
    "                    line = i.strip().split(',')\n",
    "                    feature_num = len(line) - 1\n",
    "                    for index,j in enumerate(line[1:]):\n",
    "                        if str(index) not in classification_num[line[0]]:\n",
    "                            classification_num[line[0]][str(index)] = [j]\n",
    "                        else:\n",
    "                            classification_num[line[0]][str(index)].append(j)\n",
    "                            \n",
    "            #vector_space = np.zeros((len(classification_num)+1,feature_num),np.float64) #最後一列為母體基準\n",
    "            vector_space = np.zeros((len(classification_num),feature_num),np.float64) \n",
    "\n",
    "            for k,v in classification_num.items():\n",
    "                #temp_rsd = []\n",
    "                temp_avg = []\n",
    "                for i in range(len(v)):\n",
    "                    temp_num = list(map(int,map(float,v[str(i)])))\n",
    "\n",
    "                    feature_base_value = base_value[vb+' '+fc][i]\n",
    "\n",
    "                    \n",
    "                    '''rsd = [(x-feature_base_value)**2 for x in temp_num] \n",
    "                    rsd = (sum(rsd)/len(rsd))**0.5/feature_base_value #這邊採用N而非N-1作為標準差分母\n",
    "                    #if rsd <= 1:\n",
    "                    #    rsd = 0\n",
    "                    temp_rsd.append(str(round(rsd,5)))'''\n",
    "                    \n",
    "                    avg = sum(temp_num)/len(temp_num) #計算平均，不考慮0\n",
    "                    temp_avg.append(str(round(avg-feature_base_value,5)))\n",
    "                    \n",
    "                    vector_space[int(k),i] = avg-feature_base_value\n",
    "                    \n",
    "            print (vector_space[:,(1,2)])\n",
    "                    \n",
    "            min_max_scaler = MinMaxScaler(feature_range = (0,1))\n",
    "            vector_space = min_max_scaler.fit_transform(vector_space)\n",
    "            \n",
    "            print (vector_space[:,(1,2)])\n",
    "            \n",
    "            '''for i in range(vector_space.shape[0]):\n",
    "                for j in range(vector_space.shape[1]):\n",
    "                    vector_space[i,j] = round(vector_space[i,j] - vector_space[vector_space.shape[0]-1,j],5)'''\n",
    "                    \n",
    "            #print (vector_space[:,(1,2)])\n",
    "            #print (list(map(str,list(vector_space[0]))))\n",
    "            \n",
    "            all_vector_space[vb+' '+fc+' '+cs] = vector_space\n",
    "                \n",
    "            '''with codecs.open(desktop_path+'avg_feature.csv','ab','utf8') as a:\n",
    "                for i in range(len(classification_num)):\n",
    "                    a.write(vb+' '+fc+' '+cs+' '+str(i)+','+','.join(list(map(str,list(vector_space[i]))))+'\\r\\n')'''\n",
    "            time.sleep(1)\n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC VH+N topic\n",
      "0.75769\n",
      "先進(VH)國家(Na) [[1, 2], [0]]\n",
      "國營(VH)事業(Na) [0, 1, 2]\n",
      "0.55378\n",
      "大(VH)企業(Na) [[1, 2], [0]]\n",
      "大(VH)問題(Na) [1, [0, 2]]\n",
      "小(VH)事(Na) [1, [0, 2]]\n",
      "新(VH)世紀(Na) [1, [0, 2]]\n",
      "公營(VH)事業(Na) [0, 1, 2]\n",
      "0.69543\n",
      "大(VH)飯店(Nc) [[0, 1], [2]]\n",
      "0.0411\n",
      "小(VH)狗(Na) [[0, 1], [2]]\n",
      "大(VH)醫院(Nc) [[0, 1], 2]\n",
      "重要(VH)因素(Na) [0, 1, 2]\n",
      "0.77358\n",
      "受難(VH)者(Na) [[1, 2], [0]]\n",
      "-0.21311\n",
      "密切(VH)關係(Na) [[0, 2], [1]]\n",
      "-0.38217\n",
      "自由(VH)主義(Na) [[1, 2], [0]]\n",
      "小(VH)動物(Na) [1, [0, 2]]\n",
      "大(VH)公司(Nc) [1, [0, 2]]\n",
      "小(VH)船(Na) [[0, 1], 2]\n",
      "0.23174\n",
      "大(VH)城市(Na) [[0, 2], [1]]\n",
      "不同(VH)意見(Na) [0, [1, 2]]\n",
      "大(VH)家庭(Na) [[0, 1], 2]\n",
      "0.58503\n",
      "大(VH)都市(Na) [[0, 2], [1]]\n",
      "好(VH)方法(Na) [1, [0, 2]]\n",
      "0.35345\n",
      "多(VH)人(Na) [[0, 1], [2]]\n",
      "-0.23333\n",
      "好(VH)消息(Na) [[0, 1], [2]]\n",
      "0.83529\n",
      "好(VH)話(Na) [[1, 2], [0]]\n",
      "0.18919\n",
      "退伍(VH)軍人(Na) [[0, 1], [2]]\n",
      "lei D+V topic\n",
      "0.22019\n",
      "不(D)好(VH) [[0, 1], [2]]\n",
      "0.37868\n",
      "又(D)說(VE) [[0, 1], [2]]\n",
      "0.29218\n",
      "不(D)知(VK) [[0, 1], [2]]\n",
      "0.79503\n",
      "不(D)敢(VL) [[1, 2], [0]]\n",
      "0.17658\n",
      "甚(Dfa)久(VH) [[0, 1], [2]]\n",
      "0.28306\n",
      "很(Dfa)好(VH) [[0, 1], [2]]\n",
      "0.17027\n",
      "不(D)贊成(VK) [[0, 1], [2]]\n",
      "0.04498\n",
      "不(D)願(VK) [[0, 1], [2]]\n",
      "0.14849\n",
      "來(D)談(VE) [[0, 1], [2]]\n",
      "0.29251\n",
      "續(D)整理(VC) [[0, 1], [2]]\n",
      "0.23127\n",
      "來(D)見(VE) [[0, 1], [2]]\n",
      "0.35694\n",
      "去(D)看(VC) [[0, 1], [2]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5b217182cbc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeature_select\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemp_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     '''so = sorted(similar.items(), key=lambda d:len(d[1]), reverse = True)\n\u001b[1;32m     87\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesktop_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'相似.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ab'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#目的找出 1. 類別顯著於母體 2. 類別彼此接近但又非一般\n",
    "classification_name = ['雷震','殷海光','夏道平','傅正','龍平甫','蔣勻田','朱伴耘','胡適','羅鴻詔']\n",
    "classification_topic = ['社論','文章','日記']\n",
    "\n",
    "out_feature = ['高頻','bigram','trigram','標點']\n",
    "\n",
    "for k,v in all_vector_space.items(): #逐語言特徵\n",
    "    \n",
    "    if k.split()[1] in out_feature or k.split()[2] == 'name':\n",
    "        continue\n",
    "    \n",
    "    aspect = []\n",
    "    key = k.split()[0]+' '+k.split()[1]\n",
    "    #print (key)\n",
    "    print (k)\n",
    "    \n",
    "    if k.split()[2] == 'name':\n",
    "        aspect = classification_name\n",
    "    elif k.split()[2] == 'topic':\n",
    "        aspect = classification_topic\n",
    "        \n",
    "    similar = defaultdict(list)\n",
    "    non_similar = defaultdict(list)\n",
    "    feature_class_group = defaultdict(list) #各詞彙分群組合\n",
    "        \n",
    "    feature_select = all_feature[key]\n",
    "    for i in range(v.shape[1]): #逐詞組\n",
    "        temp_list = v[:,i][:-1]\n",
    "        temp_group = []\n",
    "        \n",
    "        if max(temp_list) == min(temp_list) or max(temp_list) - min(temp_list) < 0.3: #不考慮皆為0及彼此差異不大的類別\n",
    "            continue\n",
    "        for a in range(len(temp_list)): #交叉運算\n",
    "            for b in range(len(temp_list)-a-1):\n",
    "                t = b + a + 1\n",
    "                \n",
    "                if abs(temp_list[a] - temp_list[t]) < 0.33:# and abs(temp_list[a] - temp_list[t]) != 0:\n",
    "                    #if abs(temp_list[a]) < 0.33 or abs(temp_list[t]) < 0.33:\n",
    "                    #    continue\n",
    "                    #print (feature_select[i],aspect[a],a,aspect[t],t,'相似',abs(temp_list[a] - temp_list[t]))\n",
    "                    similar[(aspect[a],aspect[t])].append(feature_select[i])                   \n",
    "                elif abs(temp_list[a] - temp_list[t]) > 0.66:\n",
    "                    #print (feature_select[i],aspect[a],a,aspect[t],t,'不相似',abs(temp_list[a] - temp_list[t]))\n",
    "                    non_similar[(aspect[a],aspect[t])].append(feature_select[i])\n",
    "        \n",
    "        #分群\n",
    "        for a in range(len(temp_list)):\n",
    "            all_check = True\n",
    "            wait_append = []\n",
    "            for b in temp_group:\n",
    "                if type(b) == int and abs(temp_list[a] - temp_list[b]) < 0.33:\n",
    "                    temp_group.remove(b)\n",
    "                    wait_append.append([b,a])\n",
    "                    all_check = False\n",
    "                elif type(b) == list:\n",
    "                    check = True\n",
    "                    for c in b:\n",
    "                        if abs(temp_list[a] - temp_list[c]) >= 0.33:\n",
    "                            check = False\n",
    "                            break\n",
    "                    if check:\n",
    "                        b.append(a)\n",
    "                        all_check = False\n",
    "            for d in wait_append:\n",
    "                temp_group.append(d)\n",
    "            if all_check:\n",
    "                temp_group.append(a)\n",
    "        \n",
    "        #找出獨特群\n",
    "        for e in temp_group:\n",
    "            if type(e) == int:\n",
    "                check = True\n",
    "                for a in range(len(temp_list)):\n",
    "                    if e != a:\n",
    "                        if abs(temp_list[a] - temp_list[e]) < 0.66:\n",
    "                            check = False\n",
    "                            break\n",
    "                if check:\n",
    "                    temp_group.remove(e)\n",
    "                    print (temp_list[e])\n",
    "                    temp_group.append([e])\n",
    "                \n",
    "        print (feature_select[i],temp_group)\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    '''so = sorted(similar.items(), key=lambda d:len(d[1]), reverse = True)\n",
    "    with codecs.open(desktop_path+'相似.csv','ab','utf8') as a:\n",
    "        \n",
    "        print ('相似')\n",
    "        index = 0\n",
    "        for x,y in so:\n",
    "            index += 1\n",
    "            if index == 20:\n",
    "                break\n",
    "            print (x,y)\n",
    "        print ()\n",
    "        \n",
    "        a.write(key+'\\r\\n')\n",
    "        for x,y in so:\n",
    "            a.write(x[0]+' '+x[1]+','+','.join(y)+'\\r\\n')\n",
    "        a.write('\\r\\n')\n",
    "         \n",
    "    so = sorted(non_similar.items(), key=lambda d:len(d[1]), reverse = True)\n",
    "    with codecs.open(desktop_path+'不相似.csv','ab','utf8') as a:\n",
    "        \n",
    "        print ('不相似')\n",
    "        index = 0\n",
    "        for x,y in so:\n",
    "            index += 1\n",
    "            if index == 20:\n",
    "                break\n",
    "            print (x,y)\n",
    "        print ()\n",
    "        \n",
    "        a.write(key+'\\r\\n')\n",
    "        for x,y in so:\n",
    "            a.write(x[0]+' '+x[1]+','+','.join(y)+'\\r\\n')\n",
    "        a.write('\\r\\n')'''\n",
    "        \n",
    "print ('END')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
