{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2017/5/8 根據「自由中國SVM作者預測」檔案內容包裝\n",
    "#2017/5/10 完成部分包裝\n",
    "#2017/5/12 完成全部包裝，加入路徑規則尋找\n",
    "#2017/5/19 各函式模塊化\n",
    "\n",
    "#建立基準特徵詞組向量 -> 將清理後的資料建成文本向量 -> 資料平衡 -> 預測模型建立 -> 評估報表產生\n",
    "#同主題不同作者、同作者不同主題\n",
    "#作者歸屬常用語言特徵：高頻詞、2-gram、3-gram、標點符號\n",
    "#本研究提出語言特徵：詞性組合、否定程度組合、情態詞組合\n",
    "\n",
    "#函式庫引入\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from collections import OrderedDict,defaultdict,Counter\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV \n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#輸入資料\n",
    "\n",
    "classification_name = ['雷震','殷海光','夏道平','傅正','龍平甫','蔣勻田','朱伴耘','胡適','羅鴻詔']\n",
    "classification_topic = ['社論','文章','日記']\n",
    "\n",
    "\n",
    "#建立作者索引，提取各作者文章內容及索引\n",
    "author_index = []\n",
    "for index,name in enumerate(classification_name): #建立作者索引\n",
    "    author_index.append((name,index))\n",
    "author_index = OrderedDict(author_index) #作者索引排序(依文本數量高到低)\n",
    "\n",
    "topic_index = []\n",
    "for index,name in enumerate(classification_topic): #建立作者索引\n",
    "    topic_index.append((name,index))\n",
    "topic_index = OrderedDict(topic_index) #作者索引排序(依文本數量高到低)\n",
    "\n",
    "condicate_author_path = \"D:\\\\課業相關\\\\論文資料\\\\論文程式\\\\condicate\\\\author\\\\\" #候選作者文本\n",
    "condicate_topic_path = \"D:\\\\課業相關\\\\論文資料\\\\論文程式\\\\condicate\\\\topic\\\\\" #候選主題文本\n",
    "SC_feature_path = \"D:\\\\課業相關\\\\論文資料\\\\論文程式\\\\language_feature\\\\最終版\\\\平衡語料庫\\\\\" #平衡語料庫語言特徵\n",
    "FC_feature_path = \"D:\\\\課業相關\\\\論文資料\\\\論文程式\\\\language_feature\\\\最終版\\\\自由中國\\\\\" #自由中國語言特徵\n",
    "lei_feature_path = \"D:\\\\課業相關\\\\論文資料\\\\論文程式\\\\language_feature\\\\最終版\\\\雷震文本\\\\\" #雷震文本語言特徵\n",
    "condicate_tree_path = \"D:\\\\課業相關\\\\論文資料\\\\論文程式\\\\預測資料\\\\condicate_path\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#輸入介面\n",
    "def find_input(find):\n",
    "    \n",
    "    feature_file_path = ''\n",
    "    feature_file_name = ''\n",
    "    condicate_path = ''\n",
    "    condicate_label = ''\n",
    "    condicate_index = ''\n",
    "\n",
    "    find = find\n",
    "\n",
    "    temp = find.split()\n",
    "    \n",
    "    if len(temp) != 3:\n",
    "        print ('請輸入正確值')\n",
    "        return False\n",
    "    \n",
    "    if temp[0] == 'SC':\n",
    "        feature_file_path = SC_feature_path\n",
    "    elif temp[0] == 'FC':\n",
    "        feature_file_path = FC_feature_path\n",
    "    elif temp[0] == 'lei':\n",
    "        feature_file_path = lei_feature_path\n",
    "    else:\n",
    "        print ('母體選項不符合')\n",
    "        return False\n",
    "    \n",
    "    feature_file_name = [file for file in os.listdir(feature_file_path) if temp[1] in file]\n",
    "    if len(feature_file_name) == 0:\n",
    "        print ('輸入的語言特徵不在範圍內')\n",
    "        return False\n",
    "    feature_file_name = feature_file_name[0]\n",
    "\n",
    "    if temp[2] == 'name':\n",
    "        condicate_path = condicate_author_path\n",
    "        condicate_label = classification_name\n",
    "        condicate_index = author_index\n",
    "    elif temp[2] == 'topic':\n",
    "        condicate_path = condicate_topic_path\n",
    "        condicate_label = classification_topic\n",
    "        condicate_index = topic_index\n",
    "    else:\n",
    "        print ('領域選項不符合')\n",
    "        return False\n",
    "        \n",
    "    return feature_file_path,feature_file_name,condicate_path,condicate_label,condicate_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#抓取候選文本，並根據特徵轉換成文本向量\n",
    "\n",
    "#抓取候選文本，回傳文章序列及各文章類別代號\n",
    "def article_get(condicate_path,condicate_label,condicate_index):\n",
    "\n",
    "    content_list = [] #所有作者文本內容(未處理)\n",
    "    article_label = [] #各文本label，也就是索引\n",
    "\n",
    "    for file in os.listdir(condicate_path):\n",
    "        with codecs.open(condicate_path+file,'rb','utf8') as f:\n",
    "\n",
    "            if file.split('_')[0] not in condicate_label:\n",
    "                continue\n",
    "\n",
    "            title = f.readline()\n",
    "            content = f.readline().strip()\n",
    "\n",
    "            content_list.append(content)\n",
    "\n",
    "            article_label.append(condicate_index[file.split('_')[0]])\n",
    "        \n",
    "    return content_list,article_label\n",
    "\n",
    "#選擇語言特徵，回傳文本向量詞組\n",
    "def feature_select(feature_file_path):\n",
    "    \n",
    "    feature = [] #特徵\n",
    "    with codecs.open(feature_file_path,'rb','utf8') as f: #抓取基準特徵\n",
    "        for i in f.readlines():\n",
    "            if '\\ufeff' in i: #去掉開頭BOM\n",
    "                i = i.replace('\\ufeff','')\n",
    "            if i.strip() != '':\n",
    "                feature.append(i.strip().split(',')[0])\n",
    "                \n",
    "    return feature\n",
    "\n",
    "#建立文本向量\n",
    "def article_vector(X_raw,feature,feature_file_name):\n",
    "    \n",
    "    bi_pos_combine = ['N+N','N+V','VH+N','D+V','情態詞']\n",
    "    more_pos_combine = ['否定','程度']\n",
    "    \n",
    "    def line_vec(line): #將文章轉換為特徵向量並回傳\n",
    "        temp_feature = defaultdict(int)\n",
    "        \n",
    "        if any(word in feature_file_name for word in bi_pos_combine): #詞性組合\n",
    "            line = [line[i]+line[i+1] for i in range(len(line)-1)] \n",
    "        elif any(word in feature_file_name for word in more_pos_combine): #2~3詞性組合\n",
    "            line = [line[i]+line[i+1] for i in range(len(line)-1)] + [line[i]+line[i+1]+line[i+2] for i in range(len(line)-2)]\n",
    "        else: #其他常用語言特徵\n",
    "            line = [line[i].split('(')[0] for i in range(len(line))]\n",
    "            \n",
    "        for i in line:\n",
    "            if i in feature:\n",
    "                temp_feature[i] += 1\n",
    "        \n",
    "        return temp_feature \n",
    "    \n",
    "    vector_space = np.zeros((len(X_raw),len(feature)),np.float64)\n",
    "        \n",
    "    for index,element in enumerate(X_raw): #依序將文章轉換為特徵向量\n",
    "        line = element.strip().split()\n",
    "        temp_feature = line_vec(line)\n",
    "\n",
    "        for i,j in enumerate(feature):\n",
    "            vector_space[index, i] = round(temp_feature[j] * 1000000 / len(line)) #取相對頻率\n",
    "            \n",
    "    return vector_space\n",
    "\n",
    "def random_balance(X,y):\n",
    "    return RandomOverSampler(random_state=0).fit_sample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#預測模型建立\n",
    "def predict_model(X,y,test_size,model,find):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=test_size, random_state=0, stratify=y)\n",
    "    \n",
    "    kernel = ''\n",
    "    \n",
    "    if model == 'RF':\n",
    "        kernel = RandomForestClassifier(n_jobs=-1, oob_score=True, \\\n",
    "                                        class_weight = 'balanced',min_samples_leaf=3,n_estimators=700,random_state=0)\n",
    "    elif model == 'SVM':\n",
    "        kernel = svm.LinearSVC(class_weight='balanced',random_state=0)\n",
    "    else:\n",
    "        print ('model error')\n",
    "        return\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    param_grid = { \n",
    "        'n_estimators': [100, 200, 500, 700], #1000和700結果差不多\n",
    "        'max_features': ['auto', 'log2'] #sqrt = auto\n",
    "    }\n",
    "      \n",
    "    clf = GridSearchCV(kernel, param_grid=param_grid, cv=10) \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print ('訓練耗費時間：',end-start,'秒')\n",
    "    #print ()\n",
    "    \n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    \n",
    "    with codecs.open('C:\\\\Users\\\\user\\\\Desktop\\\\RF result\\\\param_grid\\\\'+find+'.txt','wb','utf8') as g:\n",
    "        print (clf.best_params_)\n",
    "        g.write(str(clf.best_params_)+'\\r\\n')\n",
    "        g.write('\\r\\n')\n",
    "        #print ()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            #print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "            #      % (mean, std * 2, params))\n",
    "            g.write(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params)+'\\r\\n')\n",
    "        #print()\n",
    "        g.write('\\r\\n')\n",
    "        g.write('預測準確率：'+str(accuracy_score(y_true, y_pred))+'\\r\\n')\n",
    "        g.write('oob error rate:'+str(1-clf.best_estimator_.oob_score_)+'\\r\\n')\n",
    "        def sparsity_ratio(X):\n",
    "            return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\n",
    "        g.write(\"輸入稀疏比:\"+str(sparsity_ratio(X))+'\\r\\n')\n",
    "        g.write('\\r\\n')\n",
    "        g.write(classification_report(y_true, y_pred)+'\\r\\n')\n",
    "    \n",
    "    return clf,y_true,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#結果評估輸出    \n",
    "def predict_report(y_true, y_pred, condicate_index):    \n",
    "    \n",
    "    for name,index in condicate_index.items():\n",
    "        print (str(index)+':'+name,end=' ')\n",
    "    print ()\n",
    "    print ()\n",
    "    \n",
    "    print (classification_report(y_true, y_pred))\n",
    "    print ('預測準確率：',accuracy_score(y_true, y_pred))\n",
    "    print ()\n",
    "\n",
    "#建立混淆矩陣\n",
    "def predict_confusion_matrix(y_true, y_pred, normal, condicate_label, find):\n",
    "\n",
    "    ZF1 = FontProperties(fname='C:\\Windows\\Fonts\\kaiu.ttf', size=14)\n",
    "    def plot_confusion_matrix(cm, classes,\n",
    "                              normalize=False,\n",
    "                              title='Confusion matrix',\n",
    "                              cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45, fontproperties=ZF1)\n",
    "        plt.yticks(tick_marks, classes, fontproperties=ZF1)\n",
    "\n",
    "        if normalize:\n",
    "            #F-1 score\n",
    "            '''cm1 = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            cm2 = cm.astype('float') / cm.sum(axis=0)[:, np.newaxis]\n",
    "            cm3 = np.zeros((cm.shape[0],cm.shape[1]),np.float64)\n",
    "            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "                if cm1[i,j]+cm2[i,j] != 0.0:\n",
    "                    cm3[i,j] = (2*cm1[i,j]*cm2[i,j])/(cm1[i,j]+cm2[i,j])\n",
    "            cm = cm3'''\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] #recall\n",
    "            #cm = cm.astype('float') / cm.sum(axis=0)[:, np.newaxis] #precision\n",
    "            #print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            #print('Confusion matrix, without normalization')\n",
    "            pass\n",
    "\n",
    "        #print(cm)\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, round(cm[i, j],2),\n",
    "                     horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                     size=24 if len(condicate_label) < 5 else 14)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    if normal:\n",
    "        plot_confusion_matrix(cnf_matrix, classes=condicate_label, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    elif not normal:\n",
    "        plot_confusion_matrix(cnf_matrix, classes=condicate_label,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "        \n",
    "    plt.savefig('C:\\\\Users\\\\user\\\\Desktop\\\\RF result\\\\picture\\\\'+find+'.png', bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#輸出各作者決策樹規則\n",
    "def set_tree_path(model,condicate_label):\n",
    "    tree_path = []\n",
    "    tree_all_path = []\n",
    "\n",
    "    label_path = defaultdict(dict)\n",
    "    for i in condicate_label:\n",
    "        label_path[i] = {}\n",
    "\n",
    "    def get_code(tree, feature_names, target_names,\n",
    "                 spacer_base=\"    \"):\n",
    "        \"\"\"Produce psuedo-code for decision tree.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        tree -- scikit-leant DescisionTree.\n",
    "        feature_names -- list of feature names.\n",
    "        target_names -- list of target (class) names.\n",
    "        spacer_base -- used for spacing code (default: \"    \").\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        based on http://stackoverflow.com/a/30104792.\n",
    "        \"\"\"\n",
    "        left      = tree.tree_.children_left\n",
    "        right     = tree.tree_.children_right\n",
    "        threshold = tree.tree_.threshold\n",
    "        features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "        value = tree.tree_.value\n",
    "\n",
    "        def recurse(left, right, threshold, features, node, depth):\n",
    "            spacer = spacer_base * depth\n",
    "            if (threshold[node] != -2):\n",
    "                tree_path.append(features[node]+' <= '+str(threshold[node]))\n",
    "                if left[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            left[node], depth+1)\n",
    "                tree_path.pop()\n",
    "                tree_path.append(features[node]+' > '+str(threshold[node]))\n",
    "                if right[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            right[node], depth+1)\n",
    "                tree_path.pop()\n",
    "            else:\n",
    "                target = value[node]\n",
    "                for i, v in zip(np.nonzero(target)[1],target[np.nonzero(target)]):\n",
    "                    target_name = target_names[i]\n",
    "                    target_count = int(v)\n",
    "                    \n",
    "                    if len(tree_path) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    temp = ' & '.join(tree_path)+':'+str(target_name)+\" (\" + str(target_count) + \" examples)\"\n",
    "                    #tree_all_path.append(' and '.join(tree_path)+':'+str(target_name)+\" (\" + str(target_count) + \" examples)\")\n",
    "                    if target_count not in label_path[target_name]:\n",
    "                        label_path[target_name][target_count] = [temp]\n",
    "                    else:\n",
    "                        label_path[target_name][target_count].append(temp)\n",
    "\n",
    "        recurse(left, right, threshold, features, 0, 0)\n",
    "\n",
    "    for i in range(len(model.best_estimator_)):\n",
    "        get_code(model.best_estimator_[i], feature, condicate_label)\n",
    "        tree_path = []\n",
    "\n",
    "    for name in label_path: \n",
    "        l = sorted([i for i in label_path[name]],reverse=True)\n",
    "        with codecs.open(condicate_tree_path+name+'.txt','wb','utf8') as g:\n",
    "            for i in l:\n",
    "                g.write('#'+str(i)+'\\r\\n')\n",
    "                for e in label_path[name][i]:\n",
    "                    g.write(e+'\\r\\n')\n",
    "                g.write('\\r\\n')\n",
    "\n",
    "#觀察類別路徑規則\n",
    "def get_tree_path(fit_class,condicate_label):\n",
    "    classification_path = defaultdict(list)\n",
    "\n",
    "    #找出各路徑規則\n",
    "    for i,e in fit_class:\n",
    "        name = condicate_label[i]\n",
    "        with codecs.open(condicate_tree_path+name+'.txt','rb','utf8') as f:\n",
    "            #print (name)\n",
    "            num = ''\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()    \n",
    "\n",
    "                if line == '':\n",
    "                    continue\n",
    "                elif line[0] == '#':\n",
    "                    num = line[1:]\n",
    "                    continue\n",
    "\n",
    "                if num != '' and int(num) < 3:\n",
    "                    continue\n",
    "\n",
    "                element_temp = line.split(':')[0].split(' & ')\n",
    "\n",
    "                element = {}\n",
    "\n",
    "                for j in element_temp: #重複清理\n",
    "                    j = j.split()\n",
    "                    if (j[0],j[1]) not in element:\n",
    "                        element[(j[0],j[1])] = j[2]\n",
    "                    else:\n",
    "                        if j[1] == '<=':\n",
    "                            element[(j[0],j[1])] = str(min(float(j[2]),float(element[(j[0],j[1])])))\n",
    "                        elif j[1] == '>':\n",
    "                            element[(j[0],j[1])] = str(max(float(j[2]),float(element[(j[0],j[1])])))\n",
    "                        else:\n",
    "                            print ('ERROR')\n",
    "\n",
    "                element = [k[0]+' '+k[1]+' '+v for k,v in element.items()]\n",
    "                element.insert(0,num)\n",
    "                classification_path[name].append(element)\n",
    "\n",
    "    classification_rule = defaultdict(dict)\n",
    "\n",
    "    #建立兩兩規則\n",
    "    for i in classification_path:\n",
    "        for j in classification_path[i]:\n",
    "            comb = [i for i in itertools.combinations(j[1:],2)]\n",
    "            for k in comb:\n",
    "                if k[0] > k[1]: #避免有著順序不同的key\n",
    "                    k = (k[1],k[0])\n",
    "                if k not in classification_rule[i]:\n",
    "                    classification_rule[i][k] = int(j[0])\n",
    "                else:\n",
    "                    classification_rule[i][k] += int(j[0])\n",
    "\n",
    "            if len(comb) == 0:\n",
    "                classification_rule[i][(j[1],)] = int(j[0])\n",
    "\n",
    "    #印出\n",
    "    for i in classification_rule:\n",
    "        print (i)\n",
    "        so = sorted(classification_rule[i].items(), key=lambda d:d[1], reverse = True)\n",
    "        index = 0\n",
    "        for x,y in so:\n",
    "            index += 1\n",
    "            if index == 20:\n",
    "                break\n",
    "            print (x,y)\n",
    "        print ()\n",
    "\n",
    "        '''ee = defaultdict(str)\n",
    "        index = 1\n",
    "        with codecs.open('C:\\\\Users\\\\user\\\\Desktop\\\\node.txt','wb','utf8') as g:\n",
    "            g.write('id\\tcombine\\r\\n')\n",
    "            for k,v in classification_rule[i].items():\n",
    "                if k[0] not in ee:\n",
    "                    ee[k[0]] = index\n",
    "                    g.write(str(index)+'\\t'+k[0]+'\\r\\n')\n",
    "                    index += 1\n",
    "                if len(k) > 1 and k[1] not in ee:\n",
    "                    ee[k[1]] = index\n",
    "                    g.write(str(index)+'\\t'+k[1]+'\\r\\n')\n",
    "                    index += 1\n",
    "            g.write(str(index)+'\\tX\\r\\n')\n",
    "\n",
    "        with codecs.open('C:\\\\Users\\\\user\\\\Desktop\\\\link.txt','wb','utf8') as g:\n",
    "            g.write('source\\ttarget\\ttype\\tweight\\r\\n')\n",
    "            for x,y in so:\n",
    "                if len(x) < 2:\n",
    "                    g.write(str(ee[x[0]])+'\\t'+str(index)+'\\tUndirected\\t'+str(y)+'\\r\\n')\n",
    "                else:\n",
    "                    g.write(str(ee[x[0]])+'\\t'+str(ee[x[1]])+'\\tUndirected\\t'+str(y)+'\\r\\n')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#輸出介面\n",
    "def main(find):\n",
    "    #SC(平衡語料庫)/FC(自由中國)/lei(雷震文本)+語言特徵+name(同主題不同作者)/topic(同作者不同主題)\n",
    "    #FC N+N name\n",
    "    try:\n",
    "        feature_file_path,feature_file_name,condicate_path,condicate_label,condicate_index = find_input(find)\n",
    "    except:\n",
    "        return\n",
    "    print (find)\n",
    "    #print ()\n",
    "\n",
    "    test_size = 0.2\n",
    "    kernel = 'RF' #RF/SVM\n",
    "    threshold = 0.5\n",
    "\n",
    "    X_raw,y_raw = article_get(condicate_path,condicate_label,condicate_index)\n",
    "    feature = feature_select(feature_file_path+feature_file_name)\n",
    "    X = article_vector(X_raw,feature,feature_file_name)\n",
    "    y = np.array(y_raw)\n",
    "    \n",
    "    def sparsity_ratio(X):\n",
    "        return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\n",
    "    print(\"輸入稀疏比:\", sparsity_ratio(X))\n",
    "    \n",
    "    with codecs.open('C:\\\\Users\\\\user\\\\Desktop\\\\RF result\\\\vector\\\\'+find+'.csv','wb','utf8') as g:\n",
    "        for i in range(len(X)):\n",
    "            g.write(str(y[i])+','+','.join(list(map(str,X[i])))+'\\r\\n')\n",
    "    \n",
    "    #X,y = random_balance(X,y)\n",
    "\n",
    "    model,y_true,y_pred = predict_model(X,y,test_size,kernel,find)\n",
    "\n",
    "    #predict_report(y_true, y_pred, condicate_index)\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "    #print ('F1預測閥值：',threshold)\n",
    "    fit_class = [(i,e) for i,e in enumerate(f1) if e > threshold]\n",
    "    #print ('可預測： '+' '.join([str(condicate_label[x])+':'+str(round(y,2)) \n",
    "    #                         for x,y in fit_class]))\n",
    "    \n",
    "    print ('oob error rate:',1-model.best_estimator_.oob_score_)\n",
    "\n",
    "    predict_confusion_matrix(y_true, y_pred, False, condicate_label, find)\n",
    "    #set_tree_path(model,condicate_label)\n",
    "    #get_tree_path(fit_class,condicate_label)\n",
    "    \n",
    "    joblib.dump(model, 'C:\\\\Users\\\\user\\\\Desktop\\\\RF result\\\\model\\\\'+find+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC 高頻 topic\n",
      "輸入稀疏比: 0.14873949579831935\n",
      "訓練耗費時間： 81.74456453323364 秒\n",
      "{'max_features': 'auto', 'n_estimators': 200}\n",
      "oob error rate: 0.126315789474\n",
      "整體耗費時間： 88.68716716766357 秒\n",
      "\n",
      "SC bigram topic\n",
      "輸入稀疏比: 0.941764705882353\n",
      "訓練耗費時間： 82.19668626785278 秒\n",
      "{'max_features': 'auto', 'n_estimators': 100}\n",
      "oob error rate: 0.273684210526\n",
      "整體耗費時間： 88.96285939216614 秒\n",
      "\n",
      "SC trigram topic\n",
      "輸入稀疏比: 0.9978991596638656\n",
      "訓練耗費時間： 80.46191096305847 秒\n",
      "{'max_features': 'auto', 'n_estimators': 100}\n",
      "oob error rate: 0.642105263158\n",
      "整體耗費時間： 86.6992540359497 秒\n",
      "\n",
      "SC 標點 topic\n",
      "輸入稀疏比: 0.7389260789061387\n",
      "訓練耗費時間： 81.66045784950256 秒\n",
      "{'max_features': 'auto', 'n_estimators': 200}\n",
      "oob error rate: 0.210526315789\n",
      "整體耗費時間： 86.77537965774536 秒\n",
      "\n",
      "SC N+N topic\n",
      "輸入稀疏比: 0.9761344537815126\n",
      "訓練耗費時間： 82.12799024581909 秒\n",
      "{'max_features': 'auto', 'n_estimators': 700}\n",
      "oob error rate: 0.442105263158\n",
      "整體耗費時間： 88.59994506835938 秒\n",
      "\n",
      "SC N+V topic\n",
      "輸入稀疏比: 0.9664705882352941\n",
      "訓練耗費時間： 80.92291069030762 秒\n",
      "{'max_features': 'auto', 'n_estimators': 500}\n",
      "oob error rate: 0.368421052632\n",
      "整體耗費時間： 87.20943808555603 秒\n",
      "\n",
      "SC VH+N topic\n",
      "輸入稀疏比: 0.9877310924369748\n",
      "訓練耗費時間： 79.88968896865845 秒\n",
      "{'max_features': 'auto', 'n_estimators': 100}\n",
      "oob error rate: 0.684210526316\n",
      "整體耗費時間： 85.99207997322083 秒\n",
      "\n",
      "SC D+V topic\n",
      "輸入稀疏比: 0.8266386554621848\n",
      "訓練耗費時間： 80.35510849952698 秒\n",
      "{'max_features': 'log2', 'n_estimators': 200}\n",
      "oob error rate: 0.231578947368\n",
      "整體耗費時間： 86.39439630508423 秒\n",
      "\n",
      "SC 否定 topic\n",
      "輸入稀疏比: 0.8319327731092436\n",
      "訓練耗費時間： 81.12551140785217 秒\n",
      "{'max_features': 'log2', 'n_estimators': 500}\n",
      "oob error rate: 0.231578947368\n",
      "整體耗費時間： 91.16518878936768 秒\n",
      "\n",
      "SC 程度 topic\n",
      "輸入稀疏比: 0.9070588235294118\n",
      "訓練耗費時間： 83.08989381790161 秒\n",
      "{'max_features': 'auto', 'n_estimators': 700}\n",
      "oob error rate: 0.315789473684\n",
      "整體耗費時間： 93.86165857315063 秒\n",
      "\n",
      "SC 情態 topic\n",
      "輸入稀疏比: 0.9098319327731093\n",
      "訓練耗費時間： 80.69988870620728 秒\n",
      "{'max_features': 'auto', 'n_estimators': 100}\n",
      "oob error rate: 0.305263157895\n",
      "整體耗費時間： 86.70180058479309 秒\n",
      "\n",
      "lei 高頻 topic\n",
      "輸入稀疏比: 0.1681512605042017\n",
      "訓練耗費時間： 80.76588010787964 秒\n",
      "{'max_features': 'auto', 'n_estimators': 200}\n",
      "oob error rate: 0.157894736842\n",
      "整體耗費時間： 87.28426718711853 秒\n",
      "\n",
      "lei bigram topic\n",
      "輸入稀疏比: 0.9919327731092437\n",
      "訓練耗費時間： 81.53279042243958 秒\n",
      "{'max_features': 'auto', 'n_estimators': 100}\n",
      "oob error rate: 0.410526315789\n",
      "整體耗費時間： 88.1229236125946 秒\n",
      "\n",
      "lei trigram topic\n",
      "輸入稀疏比: 1.0\n",
      "訓練耗費時間： 81.48286294937134 秒\n",
      "{'max_features': 'auto', 'n_estimators': 700}\n",
      "oob error rate: 0.989473684211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda2\\envs\\py35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\user\\Anaconda2\\envs\\py35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整體耗費時間： 88.30072116851807 秒\n",
      "\n",
      "lei 標點 topic\n",
      "輸入稀疏比: 0.6580765639589169\n",
      "訓練耗費時間： 85.05521821975708 秒\n",
      "{'max_features': 'auto', 'n_estimators': 500}\n",
      "oob error rate: 0.242105263158\n",
      "整體耗費時間： 90.06627583503723 秒\n",
      "\n",
      "lei N+N topic\n",
      "輸入稀疏比: 0.8405042016806723\n",
      "訓練耗費時間： 81.120365858078 秒\n",
      "{'max_features': 'auto', 'n_estimators': 100}\n",
      "oob error rate: 0.326315789474\n",
      "整體耗費時間： 87.11575078964233 秒\n",
      "\n",
      "lei N+V topic\n",
      "輸入稀疏比: 0.8843697478991597\n",
      "訓練耗費時間： 80.99313068389893 秒\n",
      "{'max_features': 'auto', 'n_estimators': 100}\n",
      "oob error rate: 0.326315789474\n",
      "整體耗費時間： 87.25565123558044 秒\n",
      "\n",
      "lei VH+N topic\n",
      "輸入稀疏比: 0.9180672268907563\n",
      "訓練耗費時間： 82.93207788467407 秒\n",
      "{'max_features': 'auto', 'n_estimators': 500}\n",
      "oob error rate: 0.294736842105\n",
      "整體耗費時間： 89.27146196365356 秒\n",
      "\n",
      "lei D+V topic\n",
      "輸入稀疏比: 0.7432773109243698\n",
      "訓練耗費時間： 86.03373789787292 秒\n",
      "{'max_features': 'log2', 'n_estimators': 100}\n",
      "oob error rate: 0.231578947368\n",
      "整體耗費時間： 91.89691662788391 秒\n",
      "\n",
      "lei 否定 topic\n",
      "輸入稀疏比: 0.7548739495798319\n",
      "訓練耗費時間： 82.08481788635254 秒\n",
      "{'max_features': 'auto', 'n_estimators': 200}\n",
      "oob error rate: 0.231578947368\n",
      "整體耗費時間： 91.91036915779114 秒\n",
      "\n",
      "lei 程度 topic\n",
      "輸入稀疏比: 0.8623529411764705\n",
      "訓練耗費時間： 91.00375032424927 秒\n",
      "{'max_features': 'auto', 'n_estimators': 500}\n",
      "oob error rate: 0.263157894737\n",
      "整體耗費時間： 102.20181560516357 秒\n",
      "\n",
      "lei 情態 topic\n",
      "輸入稀疏比: 0.8451260504201681\n",
      "訓練耗費時間： 93.59823846817017 秒\n",
      "{'max_features': 'auto', 'n_estimators': 700}\n",
      "oob error rate: 0.273684210526\n",
      "整體耗費時間： 100.21772503852844 秒\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_base = ['SC','FC','lei']\n",
    "feature_condicate = ['高頻','bigram','trigram','標點','N+N','N+V','VH+N','D+V','否定','程度','情態']\n",
    "#classification_select = ['name','topic']\n",
    "classification_select = ['topic']\n",
    "\n",
    "for i in vector_base:\n",
    "    for j in feature_condicate:\n",
    "        for k in classification_select:\n",
    "            if (i != 'lei' and k == 'name') or (i != 'FC' and k == 'topic'):\n",
    "                #print (i+' '+j+' '+k)\n",
    "                start = time.time()\n",
    "                main(i+' '+j+' '+k)\n",
    "                end = time.time()\n",
    "                print ('整體耗費時間：',end-start,'秒')\n",
    "                print ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
