{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2017/4/15 完成訓練模組\n",
    "#2017/4/21 加入RUC曲線\n",
    "#考慮自由中國非文藝類，投稿數量前10名的作者\n",
    "#實驗流程：獲得內文 -> 文本特徵向量建立 -> SMOTE平衡文本量 \n",
    "# -> GridSearchCV找尋最佳參數並交叉驗證 -> 建立預測報表/點陣圖/預測模型\n",
    "#特徵採用詞性組合、情態詞+V組合、否定&程度組合\n",
    "#一般常用的特徵有高頻詞、bigram&trigram、標點符號、豐富度\n",
    "import codecs\n",
    "import os\n",
    "from collections import OrderedDict,defaultdict,Counter\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from imblearn.under_sampling import OneSidedSelection \n",
    "\n",
    "condicate_author = ['雷震','殷海光','夏道平','傅正','龍平甫','蔣勻田','朱伴耘','胡適','羅鴻詔']\n",
    "\n",
    "condicate_author_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\condicate_author\\\\\" #候選作者文本\n",
    "normal_feature_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\language_feature\\\\\" #常用特徵\n",
    "paper_feature_path = \"D:\\\\課業相關\\論文資料\\\\SVM\\\\language_feature\\\\三版\\\\\" #論文提出特徵\n",
    "unknow_author_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\unknow_author\\\\\" #未知作者文本\n",
    "\n",
    "print (\"library and path OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#建立作者索引，提取各作者文章內容及索引\n",
    "author_index = defaultdict(int)\n",
    "for index,name in enumerate(condicate_author): #建立作者索引\n",
    "    author_index[name] = index\n",
    "\n",
    "author_index = OrderedDict(sorted(author_index.items(), key=lambda t: t[1])) #作者索引排序(依文本數量高到低)\n",
    "\n",
    "for name,index in author_index.items():\n",
    "    print (name+':'+str(index),end=' ')\n",
    "print ()\n",
    "    \n",
    "content_list = [] #所有作者文本內容(未處理)\n",
    "article_label = [] #各文本label，也就是作者索引\n",
    "\n",
    "a_c = defaultdict(int)\n",
    "\n",
    "for file in os.listdir(condicate_author_path):\n",
    "    with codecs.open(condicate_author_path+file,'rb','utf8') as f:\n",
    "        \n",
    "        if file.split('_')[0] not in condicate_author:\n",
    "            continue\n",
    "        \n",
    "        '''if a_c[file.split('_')[0]] > 19:\n",
    "            continue'''\n",
    "        \n",
    "        title = f.readline()\n",
    "        content = f.readline().strip()\n",
    "        \n",
    "        '''if len(content.split()) <= 2000:\n",
    "            continue'''\n",
    "        \n",
    "        content_list.append(content)\n",
    "        \n",
    "        article_label.append(author_index[file.split('_')[0]])\n",
    "        \n",
    "        '''if len(content.split()) <= 2000:\n",
    "            content_list.append(content)\n",
    "            article_label.append(author_index[file.split('_')[0]])'''\n",
    "        \n",
    "        #a_c[file.split('_')[0]] += 1\n",
    "        \n",
    "print ('共有'+str(len(content_list))+'篇文章')\n",
    "print (Counter(article_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#建立特徵\n",
    "feature = [] #特徵\n",
    "feature_file = '自由中國前100個N+N.csv' \n",
    "select_path = paper_feature_path\n",
    "with codecs.open(select_path+feature_file,'rb','utf8') as f: #抓取基準特徵\n",
    "    for i in f.readlines():\n",
    "        if i.strip() != '':\n",
    "            feature.append(i.strip().split(',')[0])\n",
    "            \n",
    "feature = feature[:50]\n",
    "\n",
    "#單以作者間為母體\n",
    "'''feature_select = defaultdict(int)\n",
    "\n",
    "def format_check(word):\n",
    "    if '(' in word and 'CATEGORY' not in word and word != '' and word[0] != '(' and word.split('(')[1] != '':\n",
    "        return True\n",
    "    return False\n",
    "        \n",
    "def pos_check(word1,word2,pos1,pos2):\n",
    "    if word1.split('(')[1][0] == pos1 and word2.split('(')[1][0] == pos2:\n",
    "        return True\n",
    "    return False\n",
    "        \n",
    "for file in content_list:\n",
    "    content = file.split()\n",
    "    for i in range(len(content)-1):\n",
    "        if format_check(content[i]) and format_check(content[i+1]):\n",
    "            if content[i].split('(')[1][:2] == 'VH' and content[i+1].split('(')[1][0] == 'N' \\\n",
    "                    and content[i+1].split('(')[1].split(')')[0] != 'Neu':\n",
    "                feature_select[content[i]+content[i+1]] += 1\n",
    "\n",
    "feature_select = sorted(feature_select.items(), key=lambda d:d[1], reverse = True)\n",
    "\n",
    "for i,(f,n) in enumerate(feature_select):\n",
    "    if i == 100:\n",
    "        break\n",
    "    feature.append(f)'''\n",
    "    \n",
    "print (len(feature))   \n",
    "print (feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#將文章轉換為特徵向量，並平衡作者文本數量(以最多為基準)\n",
    "def line_vec(line): #將文章轉換為特徵向量並回傳\n",
    "    temp_feature = defaultdict(int)\n",
    "    \n",
    "    #line = [line[i].split('(')[0] for i in range(len(line))] #詞頻\n",
    "    #line = [line[i].split('(')[0]+line[i+1].split('(')[0] for i in range(len(line)-1)] #bigram\n",
    "    #line = [line[i].split('(')[0]+line[i+1].split('(')[0]+line[i+2].split('(')[0] for i in range(len(line)-2)] #trigram\n",
    "    line = [line[i]+line[i+1] for i in range(len(line)-1)] #詞性組合\n",
    "    #line = [line[i]+line[i+1] for i in range(len(line)-1)] + [line[i]+line[i+1]+line[i+2] for i in range(len(line)-2)] #2~3詞性組合\n",
    "    \n",
    "    for i in line:\n",
    "        if i in feature:\n",
    "            temp_feature[i] += 1\n",
    "    \n",
    "    return temp_feature\n",
    "\n",
    "aa_feature = np.zeros((len(content_list),len(feature)),np.float64)\n",
    "muti_author = defaultdict(list) #將向量分類\n",
    "\n",
    "for index,element in enumerate(content_list): #依序將文章轉換為特徵向量\n",
    "    line = element.split()\n",
    "    temp_feature = line_vec(line)\n",
    "    \n",
    "    for i,j in enumerate(feature):\n",
    "        aa_feature[index, i] = round(temp_feature[j] * 1000000 / len(line)) #取相對頻率\n",
    "        \n",
    "    muti_author[article_label[index]].append(list(aa_feature[index]))\n",
    "    \n",
    "'''for i,e in muti_author.items():\n",
    "    for j in e:\n",
    "        print (i,max(j),Counter(j))\n",
    "    print ()'''\n",
    "\n",
    "for index in range(1,len(muti_author)): #將分類後的向量內容數量平均，也就是文本數量一致\n",
    "\n",
    "    min_target = -1\n",
    "    min_num = 1000\n",
    "    \n",
    "    for i,e in muti_author.items():\n",
    "        if len(e) <= min_num:\n",
    "            min_num = len(e)\n",
    "            min_target = i\n",
    "            \n",
    "    if len(muti_author[0]) == min_num: #取到文本數量與最大等數值為止，預設最大是第0個\n",
    "        break\n",
    "    \n",
    "    X_t = muti_author[0]+muti_author[min_target]\n",
    "    y_t = [0]*len(muti_author[0])+[min_target]*len(muti_author[min_target])\n",
    "    \n",
    "    X_t, y_t = SMOTE(random_state=0).fit_sample(X_t,y_t) #SMOTE\n",
    "    muti_author[min_target] = X_t[len(muti_author[0]):]\n",
    "\n",
    "\n",
    "'''for a_i in range (len(muti_author)):\n",
    "    for index in range(a_i+1,len(muti_author)):\n",
    "\n",
    "        X_t = muti_author[a_i]+muti_author[index]\n",
    "        y_t = [a_i]*len(muti_author[a_i])+[index]*len(muti_author[index])\n",
    "\n",
    "        X_t, y_t = OneSidedSelection(random_state=0).fit_sample(X_t,y_t) #OSS\n",
    "        \n",
    "        if Counter(y_t)[a_i] <= 40 and Counter(y_t)[index] <= 40:\n",
    "            continue\n",
    "        \n",
    "        muti_author[index] = [X_t[j] for j in [i for i,e in enumerate(y_t) if e == index]]\n",
    "        muti_author[a_i] = [X_t[j] for j in [i for i,e in enumerate(y_t) if e == a_i]]\n",
    "        \n",
    "max_target = 0\n",
    "max_num = -1\n",
    "for k,v in muti_author.items():\n",
    "    if len(v) > max_num:\n",
    "        max_num = len(v)\n",
    "        max_target = k\n",
    "        \n",
    "for index in range(1,len(muti_author)): #將分類後的向量內容數量平均，也就是文本數量一致\n",
    "\n",
    "    min_target = -1\n",
    "    min_num = 1000\n",
    "    \n",
    "    for i,e in muti_author.items():\n",
    "        if len(e) <= min_num:\n",
    "            min_num = len(e)\n",
    "            min_target = i\n",
    "            \n",
    "    if len(muti_author[max_target]) == min_num: #取到文本數量與最大等數值為止，預設最大是第0個\n",
    "        break\n",
    "    \n",
    "    X_t = muti_author[max_target]+muti_author[min_target]\n",
    "    y_t = [max_target]*len(muti_author[max_target])+[min_target]*len(muti_author[min_target])\n",
    "    \n",
    "    X_t, y_t = SMOTE(random_state=0).fit_sample(X_t,y_t) #SMOTE\n",
    "    muti_author[min_target] = X_t[len(muti_author[max_target]):]'''\n",
    "\n",
    "    \n",
    "X = [] #處理後文本向量\n",
    "y = [] #處理後文本label\n",
    "\n",
    "#X,y = OneSidedSelection(random_state=0).fit_sample(aa_feature,article_label)\n",
    "\n",
    "for k,v in muti_author.items():\n",
    "    for l in v:\n",
    "        X.append(l)\n",
    "    y += [k]*len(v)\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "#X,y = OneSidedSelection(random_state=0).fit_sample(X,y)\n",
    "\n",
    "print ('X:',X.shape)\n",
    "print ('y:',y.shape)\n",
    "print ('資料準備已完成')\n",
    "print (Counter(list(y)))\n",
    "\n",
    "zero_count = []\n",
    "for i in range(len(feature)):\n",
    "    zero_count.append(Counter(X[:,i].tolist())[0]) #找出特徵中0的出現次數\n",
    "print (zero_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparsity_ratio(X):\n",
    "    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\n",
    "print(\"input sparsity ratio:\", sparsity_ratio(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "X = preprocessing.scale(X)\n",
    "print (X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#建立預測模型\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42) #分割訓練與測試資料\n",
    "\n",
    "print (feature_file)\n",
    "print ()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#MultinomialNB\n",
    "'''from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "param_grid = {\n",
    "    'fit_prior': [True,False]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(MultinomialNB(), n_jobs=-1, param_grid=param_grid, cv=10)\n",
    "clf.fit(X_train,y_train)'''\n",
    "\n",
    "\n",
    "#決策樹\n",
    "'''param_grid = {\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "tclf = DecisionTreeClassifier(max_features= 'sqrt')\n",
    "clf = GridSearchCV(tclf, param_grid=param_grid, cv=10)\n",
    "clf.fit(X_train, y_train)'''\n",
    "\n",
    "\n",
    "#SVC\n",
    "'''tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], #SVM\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}] #候選參數\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(C=1,probability=True), tuned_parameters, n_jobs=-1, cv=10) #使用交叉驗證並找出最佳參數\n",
    "clf.fit(X_train, y_train)'''\n",
    "\n",
    "#LinearSVC\n",
    "'''param_grid = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'dual': [True,False],\n",
    "    'loss': [\"squared_hinge\",\"hinge\"],\n",
    "    'penalty': [\"l1\",\"l2\"]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(svm.LinearSVC(class_weight='balanced'), param_grid=param_grid, \n",
    "                   n_jobs=-1, cv=10, error_score=0.0) #使用交叉驗證並找出最佳參數\n",
    "clf.fit(X_train, y_train)'''\n",
    "\n",
    "#隨機森林樹\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 700], #1000和700結果差不多\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1, oob_score = True, class_weight = 'balanced')\n",
    "clf = GridSearchCV(rfc, param_grid=param_grid, cv=10, error_score=0.0) #entropy沒差別\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print ('耗費時間：',end-start,'秒')\n",
    "print ()\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_) #印出最佳參數\n",
    "print()\n",
    "print ('最佳預測精準度：',clf.best_score_)\n",
    "print ()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\") \n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred)) #顯示報表(pred可能和predict_proba不一致，這邊假設為一致)\n",
    "print (accuracy_score(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "print (hamming_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#建立困惑矩陣\n",
    "import itertools\n",
    "ZF1 = FontProperties(fname='C:\\Windows\\Fonts\\kaiu.ttf', size=14)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontproperties=ZF1)\n",
    "    plt.yticks(tick_marks, classes, fontproperties=ZF1)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j],2),\n",
    "                 horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=condicate_author,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.savefig('C:\\\\Users\\\\user\\\\Desktop\\\\0001.png', bbox_inches=\"tight\")\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=condicate_author, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.savefig('C:\\\\Users\\\\user\\\\Desktop\\\\0002.png', bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#比較個演算法\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0) #分割訓練與測試資料\n",
    "\n",
    "print (feature_file)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "def NB_pre(pre):\n",
    "    clf = GridSearchCV(pre, n_jobs=-1, param_grid={}, cv=10)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return clf.best_score_\n",
    "    \n",
    "\n",
    "#測試分類\n",
    "print('Gaussian 分類:', NB_pre(gnb))\n",
    "print ()\n",
    "print('Multinomial 分類:', NB_pre(mnb))\n",
    "print ()\n",
    "print('Bernoulli 分類:', NB_pre(bnb))\n",
    "print ()\n",
    "\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "'''param_grid = {\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "tclf = DecisionTreeClassifier(max_features= 'sqrt')\n",
    "clf = GridSearchCV(tclf, param_grid=param_grid, cv=10)\n",
    "clf.fit(X_train, y_train)\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('DecisionTree 分類:', clf.best_score_)\n",
    "print ()\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 700], #1000和700結果差不多\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True)\n",
    "clf = GridSearchCV(rfc, n_jobs=-1, param_grid=param_grid, cv=10)\n",
    "clf.fit(X_train, y_train)\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('RandomForest 分類:', clf.best_score_)\n",
    "print ()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#重要特徵選取\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = SelectFromModel(clf.best_estimator_, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print (X_new.shape)\n",
    "#X = X_new\n",
    "\n",
    "'''zero_count = []\n",
    "for i in range(X_new.shape[1]):\n",
    "    zero_count.append(Counter(X_new[:,i].tolist())[0]) #找出特徵中0的出現次數\n",
    "print (zero_count)'''\n",
    "\n",
    "print ([feature[i] for i in range(len(feature)) if model.get_support()[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#未知作者預測\n",
    "print (' '.join([str(i)+'.'+e for i,e in enumerate(condicate_author)])) #印出作者\n",
    "print ()\n",
    " \n",
    "print ('最佳預測精準度：',clf.best_score_)\n",
    "print ()\n",
    "\n",
    "for file in os.listdir(unknow_author_path):\n",
    "    pred_y = np.zeros((1,len(feature)),np.float64)\n",
    "    with codecs.open(unknow_author_path+file,'rb','utf8') as f:\n",
    "        head = f.readline().strip()\n",
    "        content = f.readline().strip().split()\n",
    "\n",
    "        temp_feature = line_vec(content)\n",
    "\n",
    "        for i,j in enumerate(feature):\n",
    "            pred_y[0][i] = round(temp_feature[j] * 1000000 / len(content),3)\n",
    "\n",
    "    print (head)\n",
    "    print ('預測作者：'+str(clf.predict(pred_y)[0])+'.'+condicate_author[clf.predict(pred_y)[0]])\n",
    "    print ([round(i,3) for i in clf.predict_proba(pred_y)[0].tolist()])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#畫出點陣圖\n",
    "X_reduced = PCA(n_components=2).fit_transform(X) #使用PCA降維\n",
    "colors = ['white', 'red', 'lime', 'cyan', 'orange', 'black', 'blue', 'purple', 'yellow']\n",
    "for i in range(len(colors)):\n",
    "    if i in [4,5,6,8]:\n",
    "        continue\n",
    "    x_p = X_reduced[:, 0][y == i]\n",
    "    y_p = X_reduced[:, 1][y == i]\n",
    "    plt.scatter(x_p, y_p, c=colors[i])\n",
    "name = [condicate_author[i] for i in [0,1,2,3,7]]\n",
    "plt.legend(name, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., \\\n",
    "           prop = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14) ) #顯示中文\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title(\"PCA Scatter Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#預測模型儲存\n",
    "from sklearn.externals import joblib\n",
    "#joblib.dump(clf, 'C:\\\\Users\\\\user\\\\Desktop\\\\預測資料\\\\random forest\\\\自程.pkl') \n",
    "clf = joblib.load('C:\\\\Users\\\\user\\\\Desktop\\\\預測資料\\\\random forest\\\\自D+V.pkl') \n",
    "print (clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#印出特徵重要性\n",
    "print (len(clf.best_estimator_.feature_importances_))\n",
    "feature_score = list(clf.best_estimator_.feature_importances_)\n",
    "#print (feature_score)\n",
    "#print (feature_score.index(max(feature_score)))\n",
    "feature_value,feature_index = zip(*sorted(zip(feature_score,[i for i,e in enumerate(feature_score)]),reverse=True)) #排序\n",
    "print (feature_index[:10])\n",
    "print (feature_value[:10])\n",
    "print ([feature[i] for i in feature_index[:10]])\n",
    "#print (feature_value[:10])\n",
    "print ([i for i in zip(y,X[:,feature_index[0]])][:10])\n",
    "print ([i for i in zip(article_label,aa_feature[:,feature_index[0]])][:5])\n",
    "s = aa_feature[:,[i for i in feature_index[:10]]]\n",
    "print (s.shape)\n",
    "#print (aa_feature[:,feature_index[0]])\n",
    "'''with codecs.open('C:\\\\Users\\\\user\\\\Desktop\\\\前10名特徵向量.csv','wb','utf8') as g:\n",
    "    for i,e in enumerate(list(s)):\n",
    "        g.write(str(article_label[i])+','+','.join(list(map(str,e)))+'\\r\\n')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#AUC計算(待修正)\n",
    "#y_true2, y_pred2 = y_test, clf.predict_proba(X_test)\n",
    "#y_true2, y_pred2 = y_test, clf.predict(X_test)\n",
    "y_true2, y_pred2 = y_test, clf.decision_function(X_test)\n",
    "\n",
    "colors = ['green', 'red', 'lime', 'cyan', 'orange', 'black', 'blue', 'purple', 'yellow']\n",
    "lw = 2\n",
    "tag = []\n",
    "\n",
    "mean = 0.0\n",
    "\n",
    "for i in range(len(condicate_author)):\n",
    "    fpr, tpr,thresholds = roc_curve(y_true2, y_pred2[:,i], pos_label=i)\n",
    "    #fpr, tpr,thresholds = roc_curve([1 if j == i else 0 for j in y_true2], [1 if j == i else 0 for j in y_pred2])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[i], lw=lw)\n",
    "    tag.append('%s ROC curve (area = %0.2f)' % (condicate_author[i],roc_auc))  \n",
    "    mean += roc_auc\n",
    "    \n",
    "print (round(mean/len(condicate_author),2))\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.legend(tag, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., \\\n",
    "           prop = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)) #顯示中文\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('All author\\'s ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#印出決策樹\n",
    "#graphviz用conda安裝後要建立環境變數\n",
    "from sklearn import tree\n",
    "import pydotplus \n",
    "from IPython.display import Image \n",
    "\n",
    "dot_data = tree.export_graphviz(clf.best_estimator_[0], out_file=None,\n",
    "                                filled=True,feature_names=feature,class_names=condicate_author,\n",
    "                                 max_depth=None,proportion=True,rounded=True,special_characters=True) \n",
    "dot_data = dot_data.replace('helvetica','kaiu') #字型調換\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "#graph.write_pdf(\"C:\\\\Users\\\\user\\\\Desktop\\\\graph.pdf\") \n",
    "#Image(graph.create_png())\n",
    "graph.write_png(\"C:\\\\Users\\\\user\\\\Desktop\\\\rtree.png\")\n",
    "#[str(i) for i,e in enumerate(condicate_author)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#輸出決策樹規則(未清理)\n",
    "tree_content = []\n",
    "def get_code(tree, feature_names, target_names,\n",
    "             spacer_base=\"    \"):\n",
    "    \"\"\"Produce psuedo-code for decision tree.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-leant DescisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    target_names -- list of target (class) names.\n",
    "    spacer_base -- used for spacing code (default: \"    \").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    based on http://stackoverflow.com/a/30104792.\n",
    "    \"\"\"\n",
    "    left      = tree.tree_.children_left\n",
    "    right     = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    value = tree.tree_.value\n",
    "\n",
    "    def recurse(left, right, threshold, features, node, depth):\n",
    "        spacer = spacer_base * depth\n",
    "        if (threshold[node] != -2):\n",
    "            '''print(spacer + \"if ( \" + features[node] + \" <= \" + \\\n",
    "                  str(threshold[node]) + \" ) {\")'''\n",
    "            tree_content.append(spacer + \"if ( \" + features[node] + \" <= \" + \\\n",
    "                  str(threshold[node]) + \" ) {\")\n",
    "            if left[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            left[node], depth+1)\n",
    "            #print(spacer + \"}\\n\" + spacer +\"else {\")\n",
    "            tree_content.append(spacer + \"}\\n\" + spacer +\"else {\")\n",
    "            if right[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            right[node], depth+1)\n",
    "            #print(spacer + \"}\")\n",
    "            tree_content.append(spacer + \"}\")\n",
    "        else:\n",
    "            target = value[node]\n",
    "            for i, v in zip(np.nonzero(target)[1],\n",
    "                            target[np.nonzero(target)]):\n",
    "                target_name = target_names[i]\n",
    "                target_count = int(v)\n",
    "                '''print(spacer + \"return \" + str(target_name) + \\\n",
    "                      \" ( \" + str(target_count) + \" examples )\")'''\n",
    "                tree_content.append(spacer + \"return \" + str(target_name) + \\\n",
    "                      \" ( \" + str(target_count) + \" examples )\")\n",
    "\n",
    "    recurse(left, right, threshold, features, 0, 0)\n",
    "    \n",
    "#print (clf.best_estimator_[0].tree_.children_left)\n",
    "get_code(clf.best_estimator_[0], feature, condicate_author)\n",
    "with codecs.open('C:\\\\Users\\\\user\\\\Desktop\\\\rule.txt','wb','utf8') as g:\n",
    "    for i in tree_content:\n",
    "        g.write(i+'\\r\\n')\n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#輸出各作者決策樹規則\n",
    "tree_path = []\n",
    "tree_all_path = []\n",
    "\n",
    "author_path = defaultdict(dict)\n",
    "for i in condicate_author:\n",
    "    author_path[i] = {}\n",
    "\n",
    "def get_code(tree, feature_names, target_names,\n",
    "             spacer_base=\"    \"):\n",
    "    \"\"\"Produce psuedo-code for decision tree.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-leant DescisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    target_names -- list of target (class) names.\n",
    "    spacer_base -- used for spacing code (default: \"    \").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    based on http://stackoverflow.com/a/30104792.\n",
    "    \"\"\"\n",
    "    left      = tree.tree_.children_left\n",
    "    right     = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    value = tree.tree_.value\n",
    "\n",
    "    def recurse(left, right, threshold, features, node, depth):\n",
    "        spacer = spacer_base * depth\n",
    "        if (threshold[node] != -2):\n",
    "            tree_path.append(features[node]+' <= '+str(threshold[node]))\n",
    "            if left[node] != -1:\n",
    "                recurse(left, right, threshold, features,\n",
    "                        left[node], depth+1)\n",
    "            tree_path.pop()\n",
    "            tree_path.append(features[node]+' > '+str(threshold[node]))\n",
    "            if right[node] != -1:\n",
    "                recurse(left, right, threshold, features,\n",
    "                        right[node], depth+1)\n",
    "            tree_path.pop()\n",
    "        else:\n",
    "            target = value[node]\n",
    "            for i, v in zip(np.nonzero(target)[1],target[np.nonzero(target)]):\n",
    "                target_name = target_names[i]\n",
    "                target_count = int(v)\n",
    "                temp = ' and '.join(tree_path)+':'+str(target_name)+\" (\" + str(target_count) + \" examples)\"\n",
    "                #tree_all_path.append(' and '.join(tree_path)+':'+str(target_name)+\" (\" + str(target_count) + \" examples)\")\n",
    "                if target_count not in author_path[target_name]:\n",
    "                    author_path[target_name][target_count] = [temp]\n",
    "                else:\n",
    "                    author_path[target_name][target_count].append(temp)\n",
    "\n",
    "    recurse(left, right, threshold, features, 0, 0)\n",
    "    \n",
    "get_code(clf.best_estimator_[0], feature, condicate_author)\n",
    "'''with codecs.open('C:\\\\Users\\\\user\\\\Desktop\\\\all_path.txt','wb','utf8') as g:\n",
    "    for i in tree_all_path:\n",
    "        g.write(i+'\\r\\n')'''\n",
    "\n",
    "for name in author_path: \n",
    "    l = sorted([i for i in author_path[name]],reverse=True)\n",
    "    with codecs.open('C:\\\\Users\\\\user\\\\Desktop\\\\author_path\\\\'+name+'.txt','wb','utf8') as g:\n",
    "        for i in l:\n",
    "            g.write(str(i)+'\\r\\n')\n",
    "            for e in author_path[name][i]:\n",
    "                g.write(e+'\\r\\n')\n",
    "            g.write('\\r\\n')\n",
    "            \n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#針對mutlclass的ROC圖(嘗試中)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[i for i in range(len(condicate_author))])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "#X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(clf)\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "#y_score = classifier.fit(X_train, y_train).predict_proba(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot(fpr[0], tpr[0], label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "tag = []\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "'''plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]))'''\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"])\n",
    "tag.append('micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]))\n",
    "for i in range(n_classes):\n",
    "    '''plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(i, roc_auc[i]))'''\n",
    "    plt.plot(fpr[i], tpr[i])\n",
    "    tag.append('%s ROC curve (area = %0.2f)' % (condicate_author[i],roc_auc[i]))\n",
    "\n",
    "plt.legend(tag, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., \\\n",
    "           prop = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)) #顯示中文\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
