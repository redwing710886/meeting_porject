{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library and path OK\n"
     ]
    }
   ],
   "source": [
    "#2017/4/15 完成訓練模組\n",
    "#考慮自由中國非文藝類，投稿數量前10名的作者\n",
    "#實驗流程：獲得內文 -> 文本特徵向量建立 -> SMOTE平衡文本量 \n",
    "# -> GridSearchCV找尋最佳參數並交叉驗證 -> 建立預測報表/點陣圖/預測模型\n",
    "#特徵採用詞性組合、情態詞+V組合、否定&程度組合\n",
    "#一般常用的特徵有高頻詞、bigram&trigram、標點符號、豐富度\n",
    "import codecs\n",
    "import os\n",
    "from collections import OrderedDict,defaultdict,Counter\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "condicate_author = ['雷震','殷海光','夏道平','龍平甫','蔣勻田','傅正','朱伴耘','胡適','羅鴻詔','聶華苓']\n",
    "\n",
    "condicate_author_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\condicate_author\\\\\" #候選作者文本\n",
    "normal_feature_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\language_feature\\\\\" #常用特徵\n",
    "paper_feature_path = \"D:\\\\課業相關\\論文資料\\\\SVM\\\\language_feature\\\\二版\\\\\" #論文提出特徵\n",
    "unknow_author_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\unknow_author\\\\\" #未知作者文本\n",
    "\n",
    "print (\"library and path OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雷震:0 殷海光:1 夏道平:2 龍平甫:3 蔣勻田:4 傅正:5 朱伴耘:6 胡適:7 羅鴻詔:8 聶華苓:9 \n",
      "共有429篇文章\n"
     ]
    }
   ],
   "source": [
    "#建立作者索引，提取各作者文章內容及索引\n",
    "author_index = defaultdict(int)\n",
    "for index,name in enumerate(condicate_author): #建立作者索引\n",
    "    author_index[name] = index\n",
    "\n",
    "author_index = OrderedDict(sorted(author_index.items(), key=lambda t: t[1])) #作者索引排序(依文本數量高到低)\n",
    "\n",
    "for name,index in author_index.items():\n",
    "    print (name+':'+str(index),end=' ')\n",
    "print ()\n",
    "    \n",
    "content_list = [] #所有作者文本內容(未處理)\n",
    "article_label = [] #各文本label，也就是作者索引\n",
    "\n",
    "for file in os.listdir(condicate_author_path):\n",
    "    with codecs.open(condicate_author_path+file,'rb','utf8') as f:\n",
    "        title = f.readline()\n",
    "        content_list.append(f.readline().strip())\n",
    "        \n",
    "        article_label.append(author_index[file.split('_')[0]])\n",
    "        \n",
    "print ('共有'+str(len(content_list))+'篇文章')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (860, 100)\n",
      "y: (860,)\n",
      "資料準備已完成\n",
      "[0, 4, 4, 3, 11, 29, 35, 184, 13, 46, 14, 96, 96, 32, 35, 39, 49, 39, 75, 19, 39, 75, 97, 41, 48, 65, 49, 217, 104, 56, 454, 164, 676, 248, 60, 54, 88, 121, 271, 86, 90, 122, 150, 88, 229, 165, 167, 90, 24, 85, 72, 205, 114, 187, 137, 90, 83, 847, 77, 136, 112, 152, 73, 108, 372, 115, 335, 147, 245, 196, 83, 155, 266, 162, 137, 84, 102, 669, 201, 425, 183, 250, 205, 224, 49, 149, 83, 321, 138, 335, 237, 341, 336, 215, 363, 236, 151, 235, 222, 623]\n"
     ]
    }
   ],
   "source": [
    "#將文章轉換為特徵向量，並平衡作者文本數量(以最多為基準)\n",
    "feature = [] #特徵\n",
    "feature_file = '平衡語料庫前100高頻詞.csv'\n",
    "select_path = normal_feature_path\n",
    "with codecs.open(select_path+feature_file,'rb','utf8') as f: #抓取基準特徵\n",
    "    for i in f.readlines():\n",
    "        if i.strip() != '':\n",
    "            feature.append(i.strip().split(',')[0])\n",
    "\n",
    "def line_vec(line): #將文章轉換為特徵向量並回傳\n",
    "    temp_feature = defaultdict(int)\n",
    "    \n",
    "    line = [line[i].split('(')[0] for i in range(len(line))] #詞頻\n",
    "    #line = [line[i].split('(')[0]+line[i+1].split('(')[0] for i in range(len(line)-1)] #bigram\n",
    "    #line = [line[i].split('(')[0]+line[i+1].split('(')[0]+line[i+2].split('(')[0] for i in range(len(line)-2)] #trigram\n",
    "    #line = [line[i]+line[i+1] for i in range(len(line)-1)] #詞性組合\n",
    "    \n",
    "    for i in line:\n",
    "        if i in feature:\n",
    "            temp_feature[i] += 1\n",
    "    \n",
    "    return temp_feature\n",
    "\n",
    "aa_feature = np.zeros((len(content_list),len(feature)),np.float64)\n",
    "muti_author = defaultdict(list) #將向量分類\n",
    "\n",
    "for index,element in enumerate(content_list): #依序將文章轉換為特徵向量\n",
    "    line = element.split()\n",
    "    temp_feature = line_vec(line)\n",
    "    \n",
    "    for i,j in enumerate(feature):\n",
    "        aa_feature[index, i] = round(temp_feature[j] * 1000000 / len(line),3) #取相對頻率\n",
    "        \n",
    "    muti_author[article_label[index]].append(list(aa_feature[index]))\n",
    "\n",
    "for index in range(1,len(muti_author)): #將分類後的向量內容數量平均，也就是文本數量一致\n",
    "\n",
    "    min_target = -1\n",
    "    min_num = 99\n",
    "    \n",
    "    for i,e in muti_author.items():\n",
    "        if len(e) <= min_num:\n",
    "            min_num = len(e)\n",
    "            min_target = i\n",
    "            \n",
    "    if len(muti_author[0]) == min_num: #取到文本數量與最大等數值為止，預設最大是第0個\n",
    "        break\n",
    "    \n",
    "    X_t = muti_author[0]+muti_author[min_target]\n",
    "    y_t = [0]*len(muti_author[0])+[min_target]*len(muti_author[min_target])\n",
    "    \n",
    "    X_t, y_t = SMOTE(random_state=0).fit_sample(X_t,y_t) #SMOTE\n",
    "    muti_author[min_target] = X_t[len(muti_author[0]):]\n",
    "    \n",
    "X = [] #處理後文本向量\n",
    "y = [] #處理後文本label\n",
    "\n",
    "for k,v in muti_author.items():\n",
    "    for l in v:\n",
    "        X.append(l)\n",
    "    y += [k]*len(v)\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print ('X:',X.shape)\n",
    "print ('y:',y.shape)\n",
    "print ('資料準備已完成')\n",
    "\n",
    "zero_count = []\n",
    "for i in range(len(feature)):\n",
    "    zero_count.append(Counter(X[:,i].tolist())[0]) #找出特徵中0的出現次數\n",
    "print (zero_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用SVM預測\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0) #分割訓練與測試資料\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}] #候選參數\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(C=1,probability=True), tuned_parameters, cv=10) #使用交叉驗證並找出最佳參數\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_) #印出最佳參數\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred)) #顯示報表(pred可能和predict_proba不一致，這邊假設為一致)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#未知作者預測\n",
    "print (' '.join([str(i)+'.'+e for i,e in enumerate(condicate_author)])) #印出作者\n",
    "print ()\n",
    " \n",
    "print ('最佳預測精準度：',clf.best_score_)\n",
    "print ()\n",
    "\n",
    "for file in os.listdir(unknow_author_path):\n",
    "    pred_y = np.zeros((1,len(feature)),np.float64)\n",
    "    with codecs.open(unknow_author_path+file,'rb','utf8') as f:\n",
    "        head = f.readline().strip()\n",
    "        content = f.readline().strip().split()\n",
    "\n",
    "        temp_feature = line_vec(content)\n",
    "\n",
    "        for i,j in enumerate(feature):\n",
    "            pred_y[0][i] = round(temp_feature[j] * 1000000 / len(content),3)\n",
    "\n",
    "    print (head)\n",
    "    print ('預測作者：'+str(clf.predict(pred_y)[0])+'.'+condicate_author[clf.predict(pred_y)[0]])\n",
    "    print ([round(i,3) for i in clf.predict_proba(pred_y)[0].tolist()])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#畫出點陣圖\n",
    "X_reduced = PCA(n_components=2).fit_transform(X) #使用PCA降維\n",
    "\n",
    "colors = ['black', 'blue', 'purple', 'yellow', 'white', 'red', 'lime', 'cyan', 'orange', 'gray']\n",
    "for i in range(len(colors)):\n",
    "    '''if i in [1,3,5,7,9]:\n",
    "        continue'''\n",
    "    x_p = X_reduced[:, 0][y == i]\n",
    "    y_p = X_reduced[:, 1][y == i]\n",
    "    plt.scatter(x_p, y_p, c=colors[i])\n",
    "plt.legend(condicate_author, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., \\\n",
    "           prop = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14) ) #顯示中文\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title(\"PCA Scatter Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'C:\\\\Users\\\\user\\\\Desktop\\\\預測資料\\\\自bigram.pkl') \n",
    "n = joblib.load('C:\\\\Users\\\\user\\\\Desktop\\\\預測資料\\\\自bigram.pkl') \n",
    "print (n.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
