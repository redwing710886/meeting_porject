{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#找出各類特徵，以作為機器學習的向量\n",
    "import codecs\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "out_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\language_feature\\\\三版\\\\\"\n",
    "\n",
    "n_rule = ['(Na)','(Nb)','(Nc)','(Ncd)','(Nd)','(Nv)']\n",
    "v_rule = ['(VA)','(VAC)','(VB)','(VC)','(VCL)','(VD)','(VE)','(VF)','(VG)','(VH)','(VHC)','(VI)','(VJ)','(VK)','(VL)']\n",
    "d_rule = ['(Da)','(Dfa)','(Dfb)','(Di)','(Dk)','(D)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3000\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "#平衡語料庫/自由中國的高頻詞、標點符號、2gram及3gram\n",
    "#input_path = \"D:\\\\課業相關\\\\論文資料\\\\SCS2\\\\\"\n",
    "input_path = \"D:\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\自由中國(2nd)\\\\自由中國-非文藝類\\\\\"\n",
    "\n",
    "SCS_dic = defaultdict(int)\n",
    "SCS_category = defaultdict(int)\n",
    "SCS_bigram = defaultdict(int)\n",
    "SCS_trigram = defaultdict(int)\n",
    "\n",
    "for index,file in enumerate(os.listdir(input_path)):\n",
    "    if index % 3000 == 0:\n",
    "        print (index)\n",
    "    with codecs.open(input_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        \n",
    "        for word in content:\n",
    "            if '(' in word:\n",
    "                if 'CATEGORY' in word.split('(')[1]:\n",
    "                    SCS_category[word.split('(')[0]] += 1\n",
    "                else:\n",
    "                    SCS_dic[word.split('(')[0]] += 1\n",
    "        \n",
    "        def format_check(word):\n",
    "            if '(' in word and 'CATEGORY' not in word and word != '' and word[0] != '(':\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        for i in range(len(content)-1):\n",
    "            if format_check(content[i]) and format_check(content[i+1]):\n",
    "                SCS_bigram[content[i].split('(')[0]+content[i+1].split('(')[0]] += 1\n",
    "        \n",
    "        for i in range(len(content)-2):\n",
    "            if format_check(content[i]) and format_check(content[i+1]) and format_check(content[i+2]):\n",
    "                SCS_trigram[content[i].split('(')[0]+content[i+1].split('(')[0]+content[i+2].split('(')[0]] += 1\n",
    "                \n",
    "SCS_dic_sort = sorted(SCS_dic.items(), key=lambda d:d[1], reverse = True)    \n",
    "SCS_category_sort = sorted(SCS_category.items(), key=lambda d:d[1], reverse = True) \n",
    "\n",
    "with codecs.open(out_path+'自由中國前100高頻詞.csv','wb','utf8') as g:\n",
    "    for i,e in enumerate(SCS_dic_sort):\n",
    "        if i == 100:\n",
    "            break\n",
    "        g.write(e[0]+','+str(e[1])+'\\r\\n')\n",
    "\n",
    "with codecs.open(out_path+'自由中國前100標點符號.csv','wb','utf8') as g:\n",
    "    for i,e in enumerate(SCS_category_sort):\n",
    "        if i == 100:\n",
    "            break\n",
    "        g.write(e[0]+','+str(e[1])+'\\r\\n')\n",
    "\n",
    "SCS_bigram_sort = sorted(SCS_bigram.items(), key=lambda d:d[1], reverse = True)    \n",
    "SCS_trigram_sort = sorted(SCS_trigram.items(), key=lambda d:d[1], reverse = True) \n",
    "\n",
    "with codecs.open(out_path+'自由中國前100個bigram.csv','wb','utf8') as g:\n",
    "    for i,e in enumerate(SCS_bigram_sort):\n",
    "        if i == 100:\n",
    "            break\n",
    "        g.write(e[0]+','+str(e[1])+'\\r\\n')\n",
    "\n",
    "with codecs.open(out_path+'自由中國前100個trigram.csv','wb','utf8') as g:\n",
    "    for i,e in enumerate(SCS_trigram_sort):\n",
    "        if i == 100:\n",
    "            break\n",
    "        g.write(e[0]+','+str(e[1])+'\\r\\n')\n",
    "        \n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3000\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "#平衡語料庫/自由中國的詞性組合、情態+V組合、否定&程度+N組合\n",
    "#input_path = \"D:\\\\課業相關\\\\論文資料\\\\SCS2\\\\\"\n",
    "input_path = \"D:\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\自由中國(2nd)\\\\自由中國-非文藝類\\\\\"\n",
    "\n",
    "modal_path = \"D:\\\\課業相關\\\\論文資料\\\\謝佳玲情態詞\\\\word.txt\"\n",
    "\n",
    "modal_feature = []\n",
    "with codecs.open(modal_path,'rb','utf8') as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            modal_feature.append(line)\n",
    "\n",
    "n_n = defaultdict(int)\n",
    "n_v = defaultdict(int)\n",
    "d_v = defaultdict(int)\n",
    "vh_n = defaultdict(int)\n",
    "\n",
    "modal_v = defaultdict(int)\n",
    "\n",
    "for index,file in enumerate(os.listdir(input_path)):\n",
    "    if index % 3000 == 0:\n",
    "        print (index)\n",
    "    with codecs.open(input_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        \n",
    "        def format_check(word):\n",
    "            if '(' in word and 'CATEGORY' not in word and word != '' and word[0] != '(' and word.split('(')[1] != '':\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def pos_check(word1,word2,pos1,pos2):\n",
    "            if word1.split('(')[1][0] == pos1 and word2.split('(')[1][0] == pos2:\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def n_check(word):\n",
    "            if word.split('(')[1] == 'Ncd)':\n",
    "                if len(word.split('(')[0]) >= 2:\n",
    "                    return True\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        for i in range(len(content)-1):\n",
    "            if format_check(content[i]) and format_check(content[i+1]): #先無視[+vrr]之類，詞性無例外\n",
    "                '''if pos_check(content[i],content[i+1],'N','N') \\\n",
    "                    and content[i].split('(')[1].split(')')[0] not in ['Nf','Ng','Nep','Neqa','Neu','Nes'] \\\n",
    "                    and content[i+1].split('(')[1].split(')')[0] != 'Neu':\n",
    "                    n_n[content[i]+content[i+1]] += 1\n",
    "                elif pos_check(content[i],content[i+1],'N','V') \\\n",
    "                    and content[i].split('(')[1].split(')')[0] not in ['Nf','Ng','Nep','Neqa','Neu','Nes']:\n",
    "                    n_v[content[i]+content[i+1]] += 1\n",
    "                elif pos_check(content[i],content[i+1],'D','V') and content[i].split('(')[1].split(')')[0] != 'DE':\n",
    "                    d_v[content[i]+content[i+1]] += 1\n",
    "                elif content[i].split('(')[1][:2] == 'VH' and content[i+1].split('(')[1][0] == 'N' \\\n",
    "                    and content[i+1].split('(')[1].split(')')[0] != 'Neu':\n",
    "                    vh_n[content[i]+content[i+1]] += 1\n",
    "                    \n",
    "                if content[i].split('(')[0] in modal_feature and content[i+1].split('(')[1][0] == 'V' \\\n",
    "                    and content[i].split('(')[1].split(')')[0] != 'DE':\n",
    "                    modal_v[content[i]+content[i+1]] += 1'''\n",
    "                \n",
    "                if n_check(content[i]) and n_check(content[i+1]) and '('+content[i].split('(')[1] in n_rule \\\n",
    "                    and '('+content[i+1].split('(')[1] in n_rule:\n",
    "                    n_n[content[i]+content[i+1]] += 1\n",
    "                elif n_check(content[i]) and '('+content[i].split('(')[1] in n_rule \\\n",
    "                    and '('+content[i+1].split('(')[1] in v_rule:\n",
    "                    n_v[content[i]+content[i+1]] += 1\n",
    "                elif '('+content[i].split('(')[1] in d_rule and '('+content[i+1].split('(')[1] in v_rule: \n",
    "                    d_v[content[i]+content[i+1]] += 1\n",
    "                elif n_check(content[i+1]) and content[i].split('(')[1][:2] == 'VH' \\\n",
    "                    and '('+content[i+1].split('(')[1] in n_rule: \n",
    "                    vh_n[content[i]+content[i+1]] += 1\n",
    "                \n",
    "                if content[i].split('(')[0] in modal_feature and '('+content[i+1].split('(')[1] in v_rule \\\n",
    "                    and content[i].split('(')[1].split(')')[0] != 'DE':\n",
    "                    modal_v[content[i]+content[i+1]] += 1\n",
    "                    \n",
    "def dic_sort(dic):\n",
    "    return sorted(dic.items(), key=lambda d:d[1], reverse = True)\n",
    "\n",
    "def sort_print(file,dic):\n",
    "    with codecs.open(out_path+file,'wb','utf8') as g:\n",
    "        for i,e in enumerate(dic_sort(dic)):\n",
    "            if i == 100:\n",
    "                break\n",
    "            g.write(e[0]+','+str(e[1])+'\\r\\n')\n",
    "\n",
    "'''sort_print('平衡語料庫前100個N+N.csv',n_n)\n",
    "sort_print('平衡語料庫前100個N+V.csv',n_v)\n",
    "sort_print('平衡語料庫前100個D+V.csv',d_v)\n",
    "sort_print('平衡語料庫前100個VH+N.csv',vh_n)\n",
    "sort_print('平衡語料庫前100個情態詞+V.csv',modal_v)'''\n",
    "\n",
    "sort_print('自由中國前100個N+N.csv',n_n)\n",
    "sort_print('自由中國前100個N+V.csv',n_v)\n",
    "sort_print('自由中國前100個D+V.csv',d_v)\n",
    "sort_print('自由中國前100個VH+N.csv',vh_n)\n",
    "sort_print('自由中國前100個情態詞+V.csv',modal_v)\n",
    "\n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "#否定及程度特徵向量擷取\n",
    "input_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\language_feature\\\\未處理\\\\\"\n",
    "\n",
    "dfa = []\n",
    "\n",
    "#with codecs.open(input_path+'平衡語料庫程度.csv','rb','utf8') as f:\n",
    "with codecs.open(input_path+'自由中國程度.csv','rb','utf8') as f:\n",
    "    head = f.readline()\n",
    "    content = f.readlines()\n",
    "    dfa = [(i.strip().split(',')[0],i.strip().split(',')[1]) for i in content \\\n",
    "           if i.strip() != '' and len(i.strip().split(')')) == 3]\n",
    "\n",
    "#with codecs.open(out_path+'平衡語料庫前100個程度詞組合.csv','wb','utf8') as g:\n",
    "with codecs.open(out_path+'自由中國前100個程度詞組合.csv','wb','utf8') as g:\n",
    "    for i,(c,f) in enumerate(dfa):\n",
    "        if i >= 100:\n",
    "            break\n",
    "        g.write(c+','+str(f)+'\\r\\n')\n",
    "print ('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.雷震 1.殷海光 2.夏道平 3.龍平甫 4.蔣勻田 5.傅正 6.朱伴耘 7.胡適 8.羅鴻詔\n",
      "\n",
      "1954-05-16 10卷 - 10期 這是國民黨反省的時候 社論(雷震)(殷海光)\n",
      "預測作者：殷海光\n",
      "[0.232, 0.316, 0.107, 0.035, 0.048, 0.099, 0.011, 0.137, 0.016]\n",
      "\n",
      "1954-06-16 10卷 - 12期 立法院給憲政開一惡例 社論(雷震)(夏道平)\n",
      "預測作者：夏道平\n",
      "[0.175, 0.097, 0.33, 0.021, 0.014, 0.325, 0.006, 0.017, 0.015]\n",
      "\n",
      "1954-07-01 11卷 - 01期 諾言貴實踐 社論(雷震)(夏道平)\n",
      "預測作者：夏道平\n",
      "[0.183, 0.176, 0.302, 0.125, 0.027, 0.076, 0.009, 0.094, 0.009]\n",
      "\n",
      "1954-10-01 11卷 - 07期 又一個關係憲政的問題 社論(雷震)(夏道平)\n",
      "預測作者：殷海光\n",
      "[0.092, 0.739, 0.013, 0.002, 0.003, 0.01, 0.001, 0.139, 0.002]\n",
      "\n",
      "1958-01-01 18卷 - 01期 彈劾案與調查權 社論(雷震)(夏道平)\n",
      "預測作者：夏道平\n",
      "[0.253, 0.049, 0.4, 0.03, 0.013, 0.162, 0.013, 0.056, 0.025]\n",
      "\n",
      "1958-01-01 18卷 - 01期 為「自治」半月刊橫遭查扣而抗議 社論(雷震)(夏道平)\n",
      "預測作者：夏道平\n",
      "[0.149, 0.043, 0.409, 0.063, 0.012, 0.158, 0.022, 0.12, 0.022]\n",
      "\n",
      "1959-01-01 20卷 - 01期 本刊的十年回顧 社論(雷震)(夏道平)\n",
      "預測作者：傅正\n",
      "[0.189, 0.171, 0.125, 0.03, 0.122, 0.225, 0.012, 0.121, 0.003]\n",
      "\n",
      "1959-12-05 21卷 - 11期 開倒車─走私案移送軍法審判 社論(雷震)(夏道平)\n",
      "預測作者：雷震\n",
      "[0.495, 0.127, 0.177, 0.062, 0.015, 0.056, 0.01, 0.05, 0.007]\n",
      "\n",
      "1959-12-05 21卷 - 11期 請速停辦「大陸來臺國民調查」！ 社論(雷震)(傅正)\n",
      "預測作者：夏道平\n",
      "[0.288, 0.163, 0.313, 0.017, 0.021, 0.175, 0.002, 0.012, 0.01]\n",
      "\n",
      "1960-02-16 22卷 - 04期 敬告我們的國大代表─團結‧法統‧政治買賣 社論(雷震)(夏道平)\n",
      "預測作者：胡適\n",
      "[0.154, 0.126, 0.063, 0.023, 0.181, 0.063, 0.008, 0.239, 0.143]\n",
      "\n",
      "1960-03-16 22卷 - 06期 怎樣才使國大的紛爭平息了的 社論(雷震)(夏道平)\n",
      "預測作者：夏道平\n",
      "[0.237, 0.067, 0.338, 0.092, 0.136, 0.101, 0.003, 0.02, 0.006]\n",
      "\n",
      "1951-10-01 05卷 - 07期 言論自由的認識及其基本條件 社論(雷震)(殷海光)\n",
      "預測作者：胡適\n",
      "[0.229, 0.136, 0.193, 0.093, 0.016, 0.04, 0.007, 0.282, 0.003]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#豐富度比較\n",
    "import codecs\n",
    "import os\n",
    "from collections import OrderedDict,defaultdict,Counter\n",
    "\n",
    "condicate_author = ['雷震','殷海光','夏道平','龍平甫','蔣勻田','傅正','朱伴耘','胡適','羅鴻詔']\n",
    "\n",
    "print (' '.join([str(i)+'.'+e for i,e in enumerate(condicate_author)])) #印出作者\n",
    "print ()\n",
    "\n",
    "condicate_author_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\condicate_author\\\\\"\n",
    "\n",
    "author_richness = defaultdict(list)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for file in os.listdir(condicate_author_path):\n",
    "    with codecs.open(condicate_author_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        content = [i.split('(')[0] for i in content]\n",
    "        author_richness[file.split('_')[0]].append(len(Counter(content))/len(content))\n",
    "        X.append(len(Counter(content))/len(content))\n",
    "        y.append(file.split('_')[0])\n",
    "        \n",
    "author_richness = OrderedDict(sorted(author_richness.items(), key=lambda t: len(t[1]),reverse=True))\n",
    "        \n",
    "'''for i,e in author_richness.items():\n",
    "    print (i,len(e))\n",
    "    print ('max:',max(e))\n",
    "    print ('min:',min(e))\n",
    "    print ('mean',sum(e)/len(e))\n",
    "    print ()'''\n",
    "    \n",
    "unknow_author_path = \"D:\\\\課業相關\\\\論文資料\\\\SVM\\\\unknow_author\\\\\"\n",
    "\n",
    "for file in os.listdir(unknow_author_path):\n",
    "    with codecs.open(unknow_author_path+file,'rb','utf8') as f:\n",
    "        head = f.readline().strip()\n",
    "        content = [i.split('(')[0] for i in f.readline().strip().split()]\n",
    "        temp_richness = len(Counter(content))/len(content)\n",
    "        \n",
    "        X_m = [abs(temp_richness-i) for i in X]\n",
    "        \n",
    "        X_m, y_m = zip(*sorted(zip(X_m, y)))\n",
    "        the = sum([1/i for i in X_m[:100]])\n",
    "        \n",
    "        X_t = [1/(i*the) for i in X_m[:100]]\n",
    "        y_t = y_m[:100]\n",
    "        \n",
    "        predict_richness = defaultdict(float)\n",
    "        for name,score in zip(y_t,X_t):\n",
    "            predict_richness[name] += score\n",
    "        \n",
    "        print (head)\n",
    "        print ('預測作者：'+max(predict_richness, key=lambda i: predict_richness[i]))\n",
    "        print ([round(predict_richness[i],3) for i in condicate_author])\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VH_DE_N 61078\n",
      "\n",
      "N_N 420377\n",
      "D_V 413814\n",
      "N_V 292269\n",
      "DE_Na 284937\n",
      "Na_DE 167869\n",
      "Na_D 132363\n",
      "D_D 128252\n",
      "Neu_Nf 105988\n",
      "VH_DE 98058\n",
      "VC_Na 97546\n",
      "P_Na 82302\n",
      "VH_N 80526\n",
      "Nf_Na 69269\n",
      "D_P 65096\n",
      "P_Nc 53310\n",
      "Na_Ng 45902\n",
      "Nh_D 44648\n",
      "Na_Caa 43157\n",
      "DE_VH 40367\n",
      "VC_DE 39986\n",
      "Na_P 39957\n",
      "Nc_DE 39395\n",
      "D_SHI 39305\n",
      "Nep_Nf 37651\n",
      "Cbb_D 36286\n",
      "VJ_Na 36028\n",
      "Neu_Na 33420\n",
      "DE_VC 32742\n",
      "Caa_Na 32722\n",
      "Neqa_Na 31945\n"
     ]
    }
   ],
   "source": [
    "#input_path = \"D:\\\\課業相關\\\\論文資料\\\\SCS2\\\\\"\n",
    "input_path = \"D:\\\\課業相關\\\\論文資料\\\\雷震處理資料\\\\source\\\\自由中國(2nd)\\\\自由中國-非文藝類\\\\\"\n",
    "\n",
    "com_pos = defaultdict(int)\n",
    "VH_DE_N = 0\n",
    "\n",
    "for file in os.listdir(input_path):\n",
    "    with codecs.open(input_path+file,'rb','utf8') as f:\n",
    "        head = f.readline()\n",
    "        content = f.readline().strip().split()\n",
    "        \n",
    "        def format_check(word):\n",
    "            if '(' in word and 'CATEGORY' not in word and word != '' and word[0] != '(' and word.split('(')[1] != '':\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def n_check(word):\n",
    "            if word.split('(')[1] == 'Ncd)':\n",
    "                if len(word.split('(')[0]) >= 2:\n",
    "                    return True\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        for i in range(len(content)-1):\n",
    "            if format_check(content[i]) and format_check(content[i+1]): #先無視[+vrr]之類，詞性無例外\n",
    "                \n",
    "                if n_check(content[i]) and n_check(content[i+1]) and '('+content[i].split('(')[1] in n_rule \\\n",
    "                    and '('+content[i+1].split('(')[1] in n_rule:\n",
    "                    com_pos['N_N'] += 1\n",
    "                elif n_check(content[i]) and '('+content[i].split('(')[1] in n_rule \\\n",
    "                    and '('+content[i+1].split('(')[1] in v_rule:\n",
    "                    com_pos['N_V'] += 1\n",
    "                elif '('+content[i].split('(')[1] in d_rule and '('+content[i+1].split('(')[1] in v_rule: \n",
    "                    com_pos['D_V'] += 1\n",
    "                elif n_check(content[i+1]) and content[i].split('(')[1][:2] == 'VH' \\\n",
    "                    and '('+content[i+1].split('(')[1] in n_rule: \n",
    "                    com_pos['VH_N'] += 1\n",
    "                else:\n",
    "                    com_pos[content[i].split('(')[1].split(')')[0]+'_'+content[i+1].split('(')[1].split(')')[0]] += 1\n",
    "                    \n",
    "        for i in range(len(content)-2):\n",
    "            if format_check(content[i]) and format_check(content[i+1]) and format_check(content[i+2]):\n",
    "                if content[i].split('(')[1][:2] == 'VH' and content[i+1].split('(')[1][:2] == 'DE' \\\n",
    "                    and '('+content[i+2].split('(')[1] in n_rule:\n",
    "                    VH_DE_N += 1\n",
    "                \n",
    "                \n",
    "def dic_sort(dic):\n",
    "    return sorted(dic.items(), key=lambda d:d[1], reverse = True)\n",
    "\n",
    "print ('VH_DE_N',VH_DE_N)\n",
    "print ()\n",
    "\n",
    "def sort_print(dic):\n",
    "    for i,e in enumerate(dic_sort(dic)):\n",
    "        if i == 30:\n",
    "            break\n",
    "        print (e[0],e[1])\n",
    "        \n",
    "sort_print(com_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
